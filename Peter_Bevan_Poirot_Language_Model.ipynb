{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Peter_Bevan_Poirot_Language_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NorURxaULx20"
      },
      "source": [
        "**This project aims to use Agatha Christie's Poirot to train a word level language model that can generate text given some words as input. Pre-trained GloVe word embeddings (Newsgroup20) were implemented in an attempt to increase performance.**\r\n",
        "\r\n",
        "**The code was created on google colab and so may run smoother on there, but should be fine on other environments.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjo2yS8DL0Lu"
      },
      "source": [
        "# Prerequisits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcNqUwL26kXy"
      },
      "source": [
        "**The raw text version of 'Poirot Investigates' by Agatha Christie is downloaded from Project Guttenberg**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aChca2dOaekw",
        "outputId": "12442c1f-73bc-463d-9178-771c85702236"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 29.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 19.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 23.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 27.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.8)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=afc8e153fde974cd4705731886c85d3f3fb1f1ca9aba2f414e43e8406af418e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=aa5c8431d18c41995bff83f70b9eb173e2d5536d8b333c2b42bbf57822185723\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgLS8cAuDLOI"
      },
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "import string\r\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import Model\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Embedding\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import itertools\r\n",
        "import re\r\n",
        "from pickle import dump\r\n",
        "from pickle import load\r\n",
        "\r\n",
        "from kerastuner.tuners import RandomSearch\r\n",
        "from kerastuner.tuners import Hyperband\r\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\r\n",
        "\r\n",
        "#Setting random seeds for reproducibility\r\n",
        "import random\r\n",
        "random.seed(42)\r\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR-66itlrWHj",
        "outputId": "3679e33a-24db-47f0-acc9-71af345b07f8"
      },
      "source": [
        "!curl -O https://www.gutenberg.org/files/61262/61262-0.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  326k  100  326k    0     0   162k      0  0:00:02  0:00:02 --:--:--  162k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTYHuYTV_EnJ"
      },
      "source": [
        "**Load the encoder below if you are just wishing to use the model rather than train a new one**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx_8D2bi5I8f"
      },
      "source": [
        "**Unwanted text (publishing information etc) is deleted from the text and a new file saved as 'Poirot.txt'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95V3EoA4rcZY"
      },
      "source": [
        "#deleting unwanted text and front and back of book\r\n",
        "with open('61262-0.txt') as old, open('Poirot.txt', 'w') as new:\r\n",
        "    lines = old.readlines()\r\n",
        "    new.writelines(lines[110:-374])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7VAShBGx4i4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZzF1_Ob9-Vl",
        "outputId": "0be480aa-421f-4b6d-ec81-49ea38e44661"
      },
      "source": [
        "#opening the text file, saving into memory as `Poirot` and closing the file\r\n",
        "text = open('Poirot.txt', 'r')\r\n",
        "Poirot = text.read()\r\n",
        "text.close()\r\n",
        "\r\n",
        "#printing first 1000 characters to double check\r\n",
        "print(Poirot[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  The Adventure of “The Western Star”\n",
            "\n",
            "I was standing at the window of Poirot’s rooms looking out idly on\n",
            "the street below.\n",
            "\n",
            "“That’s queer,” I ejaculated suddenly beneath my breath.\n",
            "\n",
            "“What is, _mon ami_?” asked Poirot placidly, from the depths of\n",
            "his comfortable chair.\n",
            "\n",
            "“Deduce, Poirot, from the following facts! Here is a young lady,\n",
            "richly dressed—fashionable hat, magnificent furs. She is coming\n",
            "along slowly, looking up at the houses as she goes. Unknown to her,\n",
            "she is being shadowed by three men and a middle-aged woman. They\n",
            "have just been joined by an errand boy who points after the girl,\n",
            "gesticulating as he does so. What drama is this being played? Is\n",
            "the girl a crook, and are the shadowers detectives preparing to\n",
            "arrest her? Or are _they_ the scoundrels, and are they plotting to\n",
            "attack an innocent victim? What does the great detective say?”\n",
            "\n",
            "“The great detective, _mon ami_, chooses, as ever, the simplest\n",
            "course. He rises to see for himself.” And my friend joined me at\n",
            "the window.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oBIRQiIjMbV",
        "outputId": "df749964-5cb4-49c0-b5ca-856c400b3622"
      },
      "source": [
        "# using keras' `text_to_word_sequence` function to tokenise the text\r\n",
        "# #additional characters added to filters to take out weird quotes\r\n",
        "cleaned_tokens = text_to_word_sequence(Poirot, filters='“”’‘•!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\r\n",
        "\r\n",
        "print(f'There are {len(cleaned_tokens)} words in the cleaned version of Poirot Investigates ({len(set(cleaned_tokens))} unique words).')\r\n",
        "print('------------------------------------------------------------------------------------------------------')\r\n",
        "print(f'Sample of 10 tokens: {cleaned_tokens[:10]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 53501 words in the cleaned version of Poirot Investigates (6167 unique words).\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Sample of 10 tokens: ['the', 'adventure', 'of', 'the', 'western', 'star', 'i', 'was', 'standing', 'at']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI-4irhcDSMc",
        "outputId": "0c241165-005a-4789-e8b2-44c683712517"
      },
      "source": [
        "#make sequences of 36 tokens\r\n",
        "length = 36\r\n",
        "sequences = list()\r\n",
        "for i in range(length, len(cleaned_tokens)):\r\n",
        "\t# select sequence of tokens\r\n",
        "\tseq = cleaned_tokens[i-length:i]\r\n",
        "\t# convert into a line\r\n",
        "\tline = ' '.join(seq)\r\n",
        "\t# store\r\n",
        "\tsequences.append(line)\r\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 53465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrjN8KCC6ObB"
      },
      "source": [
        "# Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49gtHKnCec-J",
        "outputId": "1c4d71e8-46e7-4080-a29f-e3311a23c4a5"
      },
      "source": [
        "# defining tokenizer class to encode word sequences\r\n",
        "encoder = Tokenizer()\r\n",
        "encoder.fit_on_texts(sequences)\r\n",
        "encoded_lines = encoder.texts_to_sequences(sequences)\r\n",
        "print('--first line of text shown below in encoded form--')\r\n",
        "print(encoded_lines[0])\r\n",
        "#saving encoder for use at time of generation\r\n",
        "dump(encoder, open('./encoder.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--first line of text shown below in encoded form--\n",
            "[1, 1566, 5, 1, 1116, 407, 3, 10, 661, 23, 1, 279, 5, 13, 17, 830, 311, 52, 3022, 25, 1, 599, 1284, 12, 17, 1283, 3, 1114, 260, 1113, 21, 731, 38, 14, 120, 140]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoI7rpqvqQQ_",
        "outputId": "fb67adf0-f1d4-4522-fd8b-a79629cff144"
      },
      "source": [
        "word_index = encoder.word_index\r\n",
        "print('printing 10 values from the `word_index` dictionary')\r\n",
        "print('---------------------------------------------------')\r\n",
        "dict(itertools.islice(word_index.items(), 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing 10 values from the `word_index` dictionary\n",
            "---------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 4,\n",
              " 'and': 6,\n",
              " 'he': 8,\n",
              " 'i': 3,\n",
              " 'in': 7,\n",
              " 'it': 9,\n",
              " 'of': 5,\n",
              " 'the': 1,\n",
              " 'to': 2,\n",
              " 'was': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYFh3GGkylPE"
      },
      "source": [
        "# defining numer of words in vocabulary\r\n",
        "word_dim = len(set(cleaned_tokens))+1\r\n",
        "# making encoded lines into numpy array so that model can process\r\n",
        "encoded_lines = np.array(encoded_lines)\r\n",
        "\r\n",
        "# defining predictor and response variables full dataset for full training\r\n",
        "X, y = encoded_lines[:,:-1], encoded_lines[:,-1]\r\n",
        "\r\n",
        "#splitting to train and validate for tuning\r\n",
        "X_train, y_train = encoded_lines[:45000,:-1], encoded_lines[:45000,-1]\r\n",
        "X_val, y_val = encoded_lines[45000:,:-1], encoded_lines[45000:,-1]\r\n",
        "\r\n",
        "# one hot encoding response variable to fit with softmax function dense layer\r\n",
        "y = to_categorical(y, num_classes=word_dim)\r\n",
        "y_train = to_categorical(y_train, num_classes=word_dim)\r\n",
        "y_val = to_categorical(y_val, num_classes=word_dim)\r\n",
        "\r\n",
        "# saving the sequence length for later use\r\n",
        "sequence_len = X.shape[1]\r\n",
        "sequence_len_train = X_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQQkhq3J2PhF"
      },
      "source": [
        "# Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nd8y-Al1jpq",
        "outputId": "eeef0427-ee76-471a-934d-c398521c79c6"
      },
      "source": [
        "#checking shape of training data\r\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52773, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5UxfeY11jnA"
      },
      "source": [
        "# define tuner model\r\n",
        "\r\n",
        "def build_model(hp):\r\n",
        "  #sequential less flexible but using as it allows `predict_classes` later on\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  #adding embedding layer to project words into higher dimensional space\r\n",
        "  model.add(Embedding(word_dim, hp.Int('n_dimensions', 50, 200, 50), input_length=sequence_len))\r\n",
        "  \r\n",
        "  # tuning number of return sequence layers alongside number of units in each layer\r\n",
        "  for i in range(hp.Int('n_layers', 1, 3)):\r\n",
        "    model.add(LSTM(hp.Int('n_units', 64, 256, 64), return_sequences=True))\r\n",
        "\r\n",
        "  model.add(LSTM(hp.Int('n_units', 64, 256, 64)))\r\n",
        "\r\n",
        "  model.add(Dense(hp.Int('n_units', 64, 256, 64), activation='relu'))\r\n",
        "\r\n",
        "  #softmax dense layer to output probability distribution for each word\r\n",
        "  model.add(Dense(word_dim, activation='softmax'))\r\n",
        "\r\n",
        "  print(model.summary())\r\n",
        "\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5DYeWbUEoA0",
        "outputId": "f2a827e1-7805-4b22-b6b0-d2e2a589e8a3"
      },
      "source": [
        "# setting up tuner object and how long to tune for\r\n",
        "seed = 1 # seed to improve reproducability\r\n",
        "executions_per_trial = 2\r\n",
        "max_trials = 3\r\n",
        "tuner = RandomSearch(build_model, max_trials = max_trials , objective = 'val_accuracy', seed=seed, executions_per_trial = executions_per_trial, directory = './LOG_DIR_Poirot')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 35, 50)            319000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 35, 64)            29440     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6380)              414700    \n",
            "=================================================================\n",
            "Total params: 800,324\n",
            "Trainable params: 800,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnOIhydEbTzv",
        "outputId": "38d9908e-cf62-40b4-bcdd-adaf6c4287fa"
      },
      "source": [
        "tuner.search_space_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "n_dimensions (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 50, 'max_value': 200, 'step': 50, 'sampling': None}\n",
            "n_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': None}\n",
            "n_units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 64, 'sampling': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "QZUfhI6lEpTe",
        "outputId": "28877d4b-392e-474c-e065-5951db692c16"
      },
      "source": [
        "# executing search with validation data to evaluate\r\n",
        "tuner.search(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val,y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "n_dimensions      |50                |?                 \n",
            "n_layers          |3                 |?                 \n",
            "n_units           |128               |?                 \n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 35, 50)            319000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 35, 128)           91648     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 35, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 35, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6380)              823020    \n",
            "=================================================================\n",
            "Total params: 1,644,932\n",
            "Trainable params: 1,644,932\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " 84/352 [======>.......................] - ETA: 5s - loss: 7.8038 - accuracy: 0.0505"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-cd11bd5b33dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# executing search with validation data to evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/kerastuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'callbacks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/kerastuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m    140\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INvZnWTQok2G"
      },
      "source": [
        "**The best model chosen by keras tuner is printed below. A smaller model with the least layers available in the search was chosen, and a mid value of 128 units per lstm layer. Since the data is very small, perhaps the deeper more complex models were quickly overfitting.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSpmXNU8cYln"
      },
      "source": [
        "#saving best model once tuning complete\r\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTVtLhp47JHr"
      },
      "source": [
        "# Main model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyp21QTOrqCR"
      },
      "source": [
        "**After poor results using the model selected by tuning, it was decided to use pre-trained word embeddings. This would hopefully mitigate the fact that useful word embeddings may not have been learned due to the small training data**\r\n",
        "\r\n",
        "**The below code block is adapted from Keras for transfer learning of gloVe pre-trained word embeddings (Newsgroup20)**\r\n",
        "\r\n",
        "https://keras.io/examples/nlp/pretrained_word_embeddings/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsppBzO2r0ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8feb6a4-5730-4d72-ad82-da5393df6120"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "!unzip -q glove.6B.zip\r\n",
        "\r\n",
        "\r\n",
        "path_to_glove_file = \"./glove.6B.100d.txt\"\r\n",
        "\r\n",
        "embeddings_index = {}\r\n",
        "with open(path_to_glove_file) as f:\r\n",
        "    for line in f:\r\n",
        "        word, coefs = line.split(maxsplit=1)\r\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\r\n",
        "        embeddings_index[word] = coefs\r\n",
        "\r\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\r\n",
        "\r\n",
        "\r\n",
        "num_tokens = len(set(cleaned_tokens)) + 2\r\n",
        "embedding_dim = 100\r\n",
        "hits = 0\r\n",
        "misses = 0\r\n",
        "\r\n",
        "# Prepare embedding matrix\r\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\r\n",
        "for word, i in word_index.items():\r\n",
        "    embedding_vector = embeddings_index.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        # Words not found in embedding index will be all-zeros.\r\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\r\n",
        "        embedding_matrix[i] = embedding_vector\r\n",
        "        hits += 1\r\n",
        "    else:\r\n",
        "        misses += 1\r\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\r\n",
        "\r\n",
        "from keras.initializers import  Constant\r\n",
        "\r\n",
        "embedding_layer = Embedding(\r\n",
        "    num_tokens,\r\n",
        "    embedding_dim,\r\n",
        "    embeddings_initializer=Constant(embedding_matrix),\r\n",
        "    trainable=False,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-25 11:32:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-25 11:32:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-25 11:32:09--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.87MB/s    in 6m 51s  \n",
            "\n",
            "2021-02-25 11:39:01 (2.00 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Found 400000 word vectors.\n",
            "Converted 5609 words (770 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMbs2T4_ivmZ"
      },
      "source": [
        "**The chosen model is defined below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3OUL1LUpaUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006cbc94-c523-4430-a9f8-d4dd3485333e"
      },
      "source": [
        "input_layer = Input(shape=(sequence_len,))\r\n",
        "\r\n",
        "#adding embedding layer to project words into higher dimensional space\r\n",
        "embedded = embedding_layer(input_layer)\r\n",
        "\r\n",
        "lstm1 = LSTM(128, return_sequences=True)(embedded)\r\n",
        "\r\n",
        "lstm2 = LSTM(128, return_sequences=True)(lstm1)\r\n",
        "\r\n",
        "lstm3 = LSTM(128)(lstm2)\r\n",
        "\r\n",
        "dense = Dense(128, activation='relu')(lstm3)\r\n",
        "\r\n",
        "#softmax dense layer to output probability distribution for each word\r\n",
        "softmax = Dense(word_dim, activation='softmax')(dense)\r\n",
        "\r\n",
        "model = Model(inputs=input_layer, outputs=softmax)\r\n",
        "\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 35)]              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 35, 100)           638100    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 35, 128)           117248    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 35, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6168)              795672    \n",
            "=================================================================\n",
            "Total params: 1,830,700\n",
            "Trainable params: 1,192,600\n",
            "Non-trainable params: 638,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWg9R1CU7Pp8"
      },
      "source": [
        "# Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n9wakF81b0q"
      },
      "source": [
        "**The model is now trained with the full data for 200 epochs. 200 Epochs maybe overfits when evaluated with the validation set used previously, but since the model is not being evaluated on test data and accuracy, rather plausability, this doesn't matter so much and experimentation showed longer training to give more plausible results in the style of Agatha Christie.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3885cBkRwDLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20b387f-35bd-48d4-e071-1562c0c85553"
      },
      "source": [
        "model.fit(X, y, batch_size=64, epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "836/836 [==============================] - 18s 10ms/step - loss: 6.9430 - accuracy: 0.0529\n",
            "Epoch 2/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 6.4375 - accuracy: 0.0599\n",
            "Epoch 3/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 6.2053 - accuracy: 0.0662\n",
            "Epoch 4/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 6.0635 - accuracy: 0.0704\n",
            "Epoch 5/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 5.9464 - accuracy: 0.0750\n",
            "Epoch 6/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 5.8285 - accuracy: 0.0779\n",
            "Epoch 7/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 5.7416 - accuracy: 0.0819\n",
            "Epoch 8/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 5.6414 - accuracy: 0.0833\n",
            "Epoch 9/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 5.5466 - accuracy: 0.0875\n",
            "Epoch 10/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 5.4727 - accuracy: 0.0913\n",
            "Epoch 11/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 5.3626 - accuracy: 0.0966\n",
            "Epoch 12/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 5.2962 - accuracy: 0.0955\n",
            "Epoch 13/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 5.2108 - accuracy: 0.0977\n",
            "Epoch 14/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 5.1321 - accuracy: 0.1029\n",
            "Epoch 15/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 5.0418 - accuracy: 0.1071\n",
            "Epoch 16/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 5.0125 - accuracy: 0.1056\n",
            "Epoch 17/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.9200 - accuracy: 0.1127\n",
            "Epoch 18/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.8529 - accuracy: 0.1123\n",
            "Epoch 19/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 4.8017 - accuracy: 0.1140\n",
            "Epoch 20/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.7421 - accuracy: 0.1169\n",
            "Epoch 21/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 4.6824 - accuracy: 0.1192\n",
            "Epoch 22/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 4.6223 - accuracy: 0.1226\n",
            "Epoch 23/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 4.5812 - accuracy: 0.1243\n",
            "Epoch 24/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 4.5308 - accuracy: 0.1273\n",
            "Epoch 25/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.4772 - accuracy: 0.1303\n",
            "Epoch 26/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.4420 - accuracy: 0.1335\n",
            "Epoch 27/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.3983 - accuracy: 0.1357\n",
            "Epoch 28/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 4.3484 - accuracy: 0.1413\n",
            "Epoch 29/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 4.2990 - accuracy: 0.1446\n",
            "Epoch 30/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.2882 - accuracy: 0.1451\n",
            "Epoch 31/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 4.2262 - accuracy: 0.1511\n",
            "Epoch 32/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.1885 - accuracy: 0.1555\n",
            "Epoch 33/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.1665 - accuracy: 0.1586\n",
            "Epoch 34/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.1385 - accuracy: 0.1606\n",
            "Epoch 35/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 4.0952 - accuracy: 0.1657\n",
            "Epoch 36/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.0428 - accuracy: 0.1749\n",
            "Epoch 37/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 4.0252 - accuracy: 0.1771\n",
            "Epoch 38/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.9885 - accuracy: 0.1809\n",
            "Epoch 39/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 3.9439 - accuracy: 0.1881\n",
            "Epoch 40/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.9203 - accuracy: 0.1884\n",
            "Epoch 41/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 3.9109 - accuracy: 0.1898\n",
            "Epoch 42/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.8585 - accuracy: 0.1975\n",
            "Epoch 43/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.8361 - accuracy: 0.1994\n",
            "Epoch 44/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.8092 - accuracy: 0.2003\n",
            "Epoch 45/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 3.7654 - accuracy: 0.2076\n",
            "Epoch 46/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.7491 - accuracy: 0.2087\n",
            "Epoch 47/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.7419 - accuracy: 0.2102\n",
            "Epoch 48/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.7202 - accuracy: 0.2129\n",
            "Epoch 49/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.6707 - accuracy: 0.2205\n",
            "Epoch 50/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.6454 - accuracy: 0.2251\n",
            "Epoch 51/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 3.6257 - accuracy: 0.2269\n",
            "Epoch 52/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.5965 - accuracy: 0.2332\n",
            "Epoch 53/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.5669 - accuracy: 0.2370\n",
            "Epoch 54/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.5342 - accuracy: 0.2399\n",
            "Epoch 55/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 3.5107 - accuracy: 0.2434\n",
            "Epoch 56/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 3.4767 - accuracy: 0.2494\n",
            "Epoch 57/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.4507 - accuracy: 0.2505\n",
            "Epoch 58/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.4304 - accuracy: 0.2535\n",
            "Epoch 59/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.4052 - accuracy: 0.2585\n",
            "Epoch 60/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.3828 - accuracy: 0.2621\n",
            "Epoch 61/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.3834 - accuracy: 0.2634\n",
            "Epoch 62/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.3392 - accuracy: 0.2672\n",
            "Epoch 63/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.2928 - accuracy: 0.2729\n",
            "Epoch 64/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.2808 - accuracy: 0.2775\n",
            "Epoch 65/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.2597 - accuracy: 0.2818\n",
            "Epoch 66/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.2392 - accuracy: 0.2847\n",
            "Epoch 67/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.2046 - accuracy: 0.2888\n",
            "Epoch 68/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.1870 - accuracy: 0.2917\n",
            "Epoch 69/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.1719 - accuracy: 0.2953\n",
            "Epoch 70/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.1541 - accuracy: 0.2938\n",
            "Epoch 71/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.1410 - accuracy: 0.2966\n",
            "Epoch 72/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.1023 - accuracy: 0.3057\n",
            "Epoch 73/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.0843 - accuracy: 0.3062\n",
            "Epoch 74/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 3.0695 - accuracy: 0.3083\n",
            "Epoch 75/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.0364 - accuracy: 0.3149\n",
            "Epoch 76/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 3.0379 - accuracy: 0.3167\n",
            "Epoch 77/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.9937 - accuracy: 0.3219\n",
            "Epoch 78/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.9657 - accuracy: 0.3294\n",
            "Epoch 79/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.9493 - accuracy: 0.3303\n",
            "Epoch 80/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.9273 - accuracy: 0.3329\n",
            "Epoch 81/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.9057 - accuracy: 0.3335\n",
            "Epoch 82/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.8857 - accuracy: 0.3390\n",
            "Epoch 83/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.8790 - accuracy: 0.3412\n",
            "Epoch 84/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.8585 - accuracy: 0.3431\n",
            "Epoch 85/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.8553 - accuracy: 0.3404\n",
            "Epoch 86/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.8241 - accuracy: 0.3477\n",
            "Epoch 87/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.7783 - accuracy: 0.3563\n",
            "Epoch 88/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.7735 - accuracy: 0.3619\n",
            "Epoch 89/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.7554 - accuracy: 0.3607\n",
            "Epoch 90/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.7045 - accuracy: 0.3713\n",
            "Epoch 91/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.7100 - accuracy: 0.3686\n",
            "Epoch 92/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.6843 - accuracy: 0.3729\n",
            "Epoch 93/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.6455 - accuracy: 0.3797\n",
            "Epoch 94/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.6365 - accuracy: 0.3811\n",
            "Epoch 95/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.6455 - accuracy: 0.3767\n",
            "Epoch 96/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.6078 - accuracy: 0.3868\n",
            "Epoch 97/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.5794 - accuracy: 0.3956\n",
            "Epoch 98/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.5742 - accuracy: 0.3963\n",
            "Epoch 99/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.5616 - accuracy: 0.3941\n",
            "Epoch 100/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.5345 - accuracy: 0.4019\n",
            "Epoch 101/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.5025 - accuracy: 0.4063\n",
            "Epoch 102/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.4860 - accuracy: 0.4102\n",
            "Epoch 103/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.4760 - accuracy: 0.4083\n",
            "Epoch 104/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.4526 - accuracy: 0.4114\n",
            "Epoch 105/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.4404 - accuracy: 0.4177\n",
            "Epoch 106/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.4256 - accuracy: 0.4197\n",
            "Epoch 107/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.4024 - accuracy: 0.4242\n",
            "Epoch 108/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.3769 - accuracy: 0.4292\n",
            "Epoch 109/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.3815 - accuracy: 0.4298\n",
            "Epoch 110/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.3429 - accuracy: 0.4360\n",
            "Epoch 111/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.3251 - accuracy: 0.4378\n",
            "Epoch 112/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.3310 - accuracy: 0.4381\n",
            "Epoch 113/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.2913 - accuracy: 0.4461\n",
            "Epoch 114/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.2931 - accuracy: 0.4449\n",
            "Epoch 115/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.2541 - accuracy: 0.4555\n",
            "Epoch 116/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.2448 - accuracy: 0.4533\n",
            "Epoch 117/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.2245 - accuracy: 0.4586\n",
            "Epoch 118/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.2065 - accuracy: 0.4599\n",
            "Epoch 119/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.2261 - accuracy: 0.4524\n",
            "Epoch 120/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.1838 - accuracy: 0.4666\n",
            "Epoch 121/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.1596 - accuracy: 0.4679\n",
            "Epoch 122/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.1389 - accuracy: 0.4736\n",
            "Epoch 123/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.1375 - accuracy: 0.4741\n",
            "Epoch 124/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.1103 - accuracy: 0.4793\n",
            "Epoch 125/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.0947 - accuracy: 0.4827\n",
            "Epoch 126/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.0783 - accuracy: 0.4865\n",
            "Epoch 127/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.0610 - accuracy: 0.4873\n",
            "Epoch 128/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 2.0697 - accuracy: 0.4842\n",
            "Epoch 129/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.0193 - accuracy: 0.5001\n",
            "Epoch 130/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.9985 - accuracy: 0.5011\n",
            "Epoch 131/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 2.0136 - accuracy: 0.5004\n",
            "Epoch 132/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 2.0046 - accuracy: 0.5014\n",
            "Epoch 133/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.9847 - accuracy: 0.5038\n",
            "Epoch 134/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 1.9448 - accuracy: 0.5137\n",
            "Epoch 135/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 1.9127 - accuracy: 0.5220\n",
            "Epoch 136/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.9373 - accuracy: 0.5138\n",
            "Epoch 137/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.9160 - accuracy: 0.5170\n",
            "Epoch 138/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.9029 - accuracy: 0.5227\n",
            "Epoch 139/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.8944 - accuracy: 0.5227\n",
            "Epoch 140/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.8700 - accuracy: 0.5257\n",
            "Epoch 141/200\n",
            "836/836 [==============================] - 8s 10ms/step - loss: 1.8402 - accuracy: 0.5329\n",
            "Epoch 142/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.8409 - accuracy: 0.5364\n",
            "Epoch 143/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.8261 - accuracy: 0.5357\n",
            "Epoch 144/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.7966 - accuracy: 0.5420\n",
            "Epoch 145/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.8126 - accuracy: 0.5394\n",
            "Epoch 146/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.7794 - accuracy: 0.5469\n",
            "Epoch 147/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.7476 - accuracy: 0.5536\n",
            "Epoch 148/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.7873 - accuracy: 0.5424\n",
            "Epoch 149/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.7737 - accuracy: 0.5449\n",
            "Epoch 150/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.7414 - accuracy: 0.5574\n",
            "Epoch 151/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.7267 - accuracy: 0.5576\n",
            "Epoch 152/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.6857 - accuracy: 0.5669\n",
            "Epoch 153/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.6756 - accuracy: 0.5669\n",
            "Epoch 154/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.6736 - accuracy: 0.5712\n",
            "Epoch 155/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.6731 - accuracy: 0.5707\n",
            "Epoch 156/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.6440 - accuracy: 0.5759\n",
            "Epoch 157/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.6267 - accuracy: 0.5809\n",
            "Epoch 158/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.6158 - accuracy: 0.5841\n",
            "Epoch 159/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.6473 - accuracy: 0.5763\n",
            "Epoch 160/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.5777 - accuracy: 0.5934\n",
            "Epoch 161/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.5776 - accuracy: 0.5917\n",
            "Epoch 162/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.5543 - accuracy: 0.5949\n",
            "Epoch 163/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.5720 - accuracy: 0.5908\n",
            "Epoch 164/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.5436 - accuracy: 0.5964\n",
            "Epoch 165/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.5273 - accuracy: 0.6000\n",
            "Epoch 166/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.5200 - accuracy: 0.6038\n",
            "Epoch 167/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.4861 - accuracy: 0.6146\n",
            "Epoch 168/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.5164 - accuracy: 0.6037\n",
            "Epoch 169/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.4791 - accuracy: 0.6135\n",
            "Epoch 170/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.4895 - accuracy: 0.6063\n",
            "Epoch 171/200\n",
            "836/836 [==============================] - 10s 11ms/step - loss: 1.4743 - accuracy: 0.6134\n",
            "Epoch 172/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.4735 - accuracy: 0.6136\n",
            "Epoch 173/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.4286 - accuracy: 0.6279\n",
            "Epoch 174/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.4275 - accuracy: 0.6258\n",
            "Epoch 175/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.4450 - accuracy: 0.6159\n",
            "Epoch 176/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.4001 - accuracy: 0.6292\n",
            "Epoch 177/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.3980 - accuracy: 0.6323\n",
            "Epoch 178/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.4014 - accuracy: 0.6304\n",
            "Epoch 179/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.3783 - accuracy: 0.6346\n",
            "Epoch 180/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.3722 - accuracy: 0.6358\n",
            "Epoch 181/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.3475 - accuracy: 0.6440\n",
            "Epoch 182/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.4021 - accuracy: 0.6281\n",
            "Epoch 183/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.3356 - accuracy: 0.6427\n",
            "Epoch 184/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.3040 - accuracy: 0.6541\n",
            "Epoch 185/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.3035 - accuracy: 0.6520\n",
            "Epoch 186/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.3175 - accuracy: 0.6482\n",
            "Epoch 187/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.2790 - accuracy: 0.6581\n",
            "Epoch 188/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.2606 - accuracy: 0.6669\n",
            "Epoch 189/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.3090 - accuracy: 0.6506\n",
            "Epoch 190/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.2706 - accuracy: 0.6615\n",
            "Epoch 191/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.2767 - accuracy: 0.6583\n",
            "Epoch 192/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.2263 - accuracy: 0.6736\n",
            "Epoch 193/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.2386 - accuracy: 0.6679\n",
            "Epoch 194/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.2013 - accuracy: 0.6785\n",
            "Epoch 195/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.2379 - accuracy: 0.6676\n",
            "Epoch 196/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.2319 - accuracy: 0.6676\n",
            "Epoch 197/200\n",
            "836/836 [==============================] - 9s 10ms/step - loss: 1.2406 - accuracy: 0.6625\n",
            "Epoch 198/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.2142 - accuracy: 0.6736\n",
            "Epoch 199/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.1644 - accuracy: 0.6845\n",
            "Epoch 200/200\n",
            "836/836 [==============================] - 9s 11ms/step - loss: 1.1836 - accuracy: 0.6802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2b1236c790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoF2aFAn0vJy"
      },
      "source": [
        "# saving model weights to allow loading for predictions at later date\r\n",
        "model.save_weights('./Poirot.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqkltUVW7S2D"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxKV7Nunsqfe"
      },
      "source": [
        "**Load the trained weights to the model here if you are not using the model right after training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuxipYT29JkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37751d13-db08-46d3-ac45-8eb155637a85"
      },
      "source": [
        "#loading model weights from previous training run\r\n",
        "model.load_weights('./Poirot.h5')\r\n",
        "print('--------------------')\r\n",
        "print('model weights loaded')\r\n",
        "print('--------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "model weights loaded\n",
            "--------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4pLhrqzszsw"
      },
      "source": [
        "**The final model can be examined below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wnvauEwRD54"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6faNKrus7Fr"
      },
      "source": [
        "**The previously used encoder is loaded below to ensure consistency**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA8CezbsLl_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6109b968-e112-4180-aaba-5a39c0c5cb32"
      },
      "source": [
        "#loading the encoder used previously\r\n",
        "encoder = load(open('encoder.pkl', 'rb'))\r\n",
        "print('--------------------')\r\n",
        "print('encoder loaded')\r\n",
        "print('--------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "encoder loaded\n",
            "--------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9MJkkAEtPdl"
      },
      "source": [
        "**Two functions are written: `predict_next()` and `generate_text()`**\r\n",
        "\r\n",
        "**`predict_next()` takes a sequence of words of any length (pads to 35, so 35+ is best) as input and predicts the next word**\r\n",
        "\r\n",
        "**`generate_text()` takes a sequence of words of any length (pads to 35, so 35+ is best) as input, and outputs a generated sequence of length specified by the user.**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouDbiUr-t6dI"
      },
      "source": [
        "import builtins\r\n",
        "def predict_next():\r\n",
        "  #taking input as seed\r\n",
        "  seed=str(builtins.input('please enter some words (preferably 35 or more) as input to the model: '))\r\n",
        "  #cleaning input seed in same way as training text cleaned\r\n",
        "  seed=text_to_word_sequence(seed, filters='“”•!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\r\n",
        "\r\n",
        "  #displaying seed\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "  print(f'input_seed = {seed}')\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "\r\n",
        "  #encoding seed with encoder used for training\r\n",
        "  seed_encoded = encoder.texts_to_sequences([seed])[0]\r\n",
        "  #padding the seed so that inputs of varying sizes become 3 (longer takes last 3 words)\r\n",
        "  seed_encoded = pad_sequences([seed_encoded], maxlen=sequence_len, truncating='pre')\r\n",
        "  #prdicting next word using model\r\n",
        "  next = np.argmax(model.predict(seed_encoded), axis=-1)\r\n",
        "  next_word = ''\r\n",
        "  for word, index in encoder.word_index.items():\r\n",
        "    if index == next:\r\n",
        "      next_word = word\r\n",
        "      break\r\n",
        "  print(f'Generated next word: *{next_word}*')\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "  print(f'New text: {\" \".join(seed)} *{next_word}*')\r\n",
        "\r\n",
        "def generate_text():\r\n",
        "  #defining empty list for text to be stored in iteratively\r\n",
        "  text = list()\r\n",
        "  #taking input as seed\r\n",
        "  input_seed = str(builtins.input('please enter some words (preferably 35 or more) as input to the model: '))\r\n",
        "  #use input of number of words for generated sentence (validated)\r\n",
        "  while True: \r\n",
        "      try:\r\n",
        "          text_len = int(builtins.input(\"Please enter a number for the length of text to be generated: \"))\r\n",
        "          break\r\n",
        "      except ValueError:\r\n",
        "          print(\"Please enter a number...\")\r\n",
        "  #cleaning input seed in same way as training text cleaned\r\n",
        "  seed=text_to_word_sequence(input_seed, filters='“”•!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\r\n",
        "\r\n",
        "  #displaying seed input\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "  print(f'input seed = {input_seed}')\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "\r\n",
        "  # iteratively predicting and appending new words to seed to generate text\r\n",
        "  for i in range(text_len):\r\n",
        "    #encoding seed using generator used previously\r\n",
        "    seed_encoded = encoder.texts_to_sequences([seed])[0]\r\n",
        "    seed_encoded = pad_sequences([seed_encoded], maxlen=sequence_len, truncating='pre')\r\n",
        "    next = np.argmax(model.predict(seed_encoded), axis=-1)\r\n",
        "    next_word = ''\r\n",
        "    for word, index in encoder.word_index.items():\r\n",
        "      if index == next:\r\n",
        "        next_word = word\r\n",
        "        break\r\n",
        "    seed += ' ' + ''.join(next_word)\r\n",
        "    text.append(next_word)\r\n",
        "  print(f'Generated next word: *{text}*')\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "  print(f\"\"\"New text: {input_seed + \" \" + \" \".join(text)}\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmFd0iOtuwhr"
      },
      "source": [
        "**Testing out the `predict_next()` function (run code and you will be prompted)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KYBShjIrbYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83865a10-cd8f-43cc-8f7b-8c0c52d3750e"
      },
      "source": [
        "predict_next()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "please enter some words (preferably 35 or more) as input to the model: the hand poirot led the way out of the flat and down the stairs our captive followed and i brought up the rear with the revolver when we were out in the street poirot turned to\n",
            "--------------------------------------------------------------\n",
            "input_seed = ['the', 'hand', 'poirot', 'led', 'the', 'way', 'out', 'of', 'the', 'flat', 'and', 'down', 'the', 'stairs', 'our', 'captive', 'followed', 'and', 'i', 'brought', 'up', 'the', 'rear', 'with', 'the', 'revolver', 'when', 'we', 'were', 'out', 'in', 'the', 'street', 'poirot', 'turned', 'to']\n",
            "--------------------------------------------------------------\n",
            "Generated next word: *me*\n",
            "--------------------------------------------------------------\n",
            "New text: the hand poirot led the way out of the flat and down the stairs our captive followed and i brought up the rear with the revolver when we were out in the street poirot turned to *me*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAkCOD5Qu3yt"
      },
      "source": [
        "**Testing out the `generate_text()` function (run code and you will be prompted)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmGYIzbbrcty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38222166-2e51-4e3e-e375-4730e5f42bb5"
      },
      "source": [
        "generate_text()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "please enter some words (preferably 35 or more) as input to the model: and my friend hastings looks at me with eyes of reproach but it was all so simple it was clear from the first that out of several hundred probably applicants for no 4 montagu mansions only\n",
            "Please enter a number for the length of text to be generated: 35\n",
            "--------------------------------------------------------------\n",
            "input seed = and my friend hastings looks at me with eyes of reproach but it was all so simple it was clear from the first that out of several hundred probably applicants for no 4 montagu mansions only\n",
            "--------------------------------------------------------------\n",
            "Generated next word: *['the', 'in', 'the', 'poirot', 'one', 'you', 'after', 'as', 'minister', 'what', 'd', 'of', 'he', 'that', 'white', 'ourselves', 'o', 'with', 'the', 'on', 'imaginary', 'of', 'he', 'i', 'don', 'the', 'my', 'thief', 'the', 'poirot', 'yardly', 'a', 'little', 'enough', 'her']*\n",
            "--------------------------------------------------------------\n",
            "New text: and my friend hastings looks at me with eyes of reproach but it was all so simple it was clear from the first that out of several hundred probably applicants for no 4 montagu mansions only the in the poirot one you after as minister what d of he that white ourselves o with the on imaginary of he i don the my thief the poirot yardly a little enough her\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSNd_6MJkMuS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c99ea23-732e-4b8c-ff9b-a311f740571e"
      },
      "source": [
        "sequences[16000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'and my friend hastings looks at me with eyes of reproach but it was all so simple it was clear from the first that out of several hundred probably applicants for no 4 montagu mansions only'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXZjbldidZiA"
      },
      "source": [
        "Christie, A., 2021. Poirot Investigates by Agatha Christie. [online] Project Gutenberg. Available at: <http://www.gutenberg.org/ebooks/61262> [Accessed 16 February 2021].\r\n",
        "\r\n",
        "Team, K., 2021. Keras documentation: Using pre-trained word embeddings. [online] Keras.io. Available at: <https://keras.io/examples/nlp/pretrained_word_embeddings/> [Accessed 21 February 2021].\r\n",
        "\r\n",
        "Brownlee, J., 2021. How to Develop a Word-Level Neural Language Model and Use it to Generate Text. [online] Machine Learning Mastery. Available at: <https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/> [Accessed 17 February 2021].\r\n",
        "\r\n",
        "Medium. 2021. Next Word Prediction: A Complete Guide. [online] Available at: <https://medium.com/linagoralabs/next-word-prediction-a-complete-guide-d2e69a7a09e6> [Accessed 24 February 2021]."
      ]
    }
  ]
}