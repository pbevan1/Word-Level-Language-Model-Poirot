{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNz9ncXVnJGril9jGStJ5oy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbevan1/Word-Level-Language-Model-Poirot/blob/main/poirot_recurrent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcNqUwL26kXy"
      },
      "source": [
        "**The raw text version of 'Poirot Investigates' by Agatha Christie is downloaded from Project Guttenberg**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgLS8cAuDLOI"
      },
      "source": [
        "import numpy as np\r\n",
        "import string\r\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\r\n",
        "from keras.utils import to_categorical\r\n",
        "import itertools\r\n",
        "import re"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR-66itlrWHj",
        "outputId": "d068007f-5877-4fa3-d6c1-66c6957a26f5"
      },
      "source": [
        "!curl -O https://www.gutenberg.org/files/61262/61262-0.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  326k  100  326k    0     0   565k      0 --:--:-- --:--:-- --:--:--  565k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx_8D2bi5I8f"
      },
      "source": [
        "**Unwanted text (publishing information etc) is deleted from the text and a new file saved as 'Poirot.txt'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95V3EoA4rcZY"
      },
      "source": [
        "#deleting unwanted text and front and back of book\r\n",
        "with open('61262-0.txt') as old, open('Poirot.txt', 'w') as new:\r\n",
        "    lines = old.readlines()\r\n",
        "    new.writelines(lines[110:-374])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7VAShBGx4i4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZzF1_Ob9-Vl",
        "outputId": "306612d3-a0b5-4dde-cdc5-8481d6d225e4"
      },
      "source": [
        "#opening the text file, saving into memory as `Poirot` and closing the file\r\n",
        "text = open('Poirot.txt', 'r')\r\n",
        "Poirot = text.read()\r\n",
        "text.close()\r\n",
        "\r\n",
        "#printing first 1000 characters to double check\r\n",
        "print(Poirot[:1000])"
      ],
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  The Adventure of “The Western Star”\n",
            "\n",
            "I was standing at the window of Poirot’s rooms looking out idly on\n",
            "the street below.\n",
            "\n",
            "“That’s queer,” I ejaculated suddenly beneath my breath.\n",
            "\n",
            "“What is, _mon ami_?” asked Poirot placidly, from the depths of\n",
            "his comfortable chair.\n",
            "\n",
            "“Deduce, Poirot, from the following facts! Here is a young lady,\n",
            "richly dressed—fashionable hat, magnificent furs. She is coming\n",
            "along slowly, looking up at the houses as she goes. Unknown to her,\n",
            "she is being shadowed by three men and a middle-aged woman. They\n",
            "have just been joined by an errand boy who points after the girl,\n",
            "gesticulating as he does so. What drama is this being played? Is\n",
            "the girl a crook, and are the shadowers detectives preparing to\n",
            "arrest her? Or are _they_ the scoundrels, and are they plotting to\n",
            "attack an innocent victim? What does the great detective say?”\n",
            "\n",
            "“The great detective, _mon ami_, chooses, as ever, the simplest\n",
            "course. He rises to see for himself.” And my friend joined me at\n",
            "the window.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oBIRQiIjMbV",
        "outputId": "69e7be7e-40c2-4da9-ef16-d5d6e5a8e0c7"
      },
      "source": [
        "# using keras' `text_to_word_sequence` function to tokenise the text\r\n",
        "# #additional characters added to filters to take out weird quotes\r\n",
        "cleaned_tokens = text_to_word_sequence(Poirot, filters='“”•!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\r\n",
        "\r\n",
        "# def clean_doc(doc):\r\n",
        "# \t# replace '--' with a space ' '\r\n",
        "# \tdoc = doc.replace('--', ' ')\r\n",
        "# \t# split into tokens by white space\r\n",
        "# \ttokens = doc.split()\r\n",
        "# \t# remove punctuation from each token\r\n",
        "# \ttable = str.maketrans('', '', string.punctuation)\r\n",
        "# \ttokens = [w.translate(table) for w in tokens]\r\n",
        "# \t# remove remaining tokens that are not alphabetic\r\n",
        "# \ttokens = [word for word in tokens if word.isalpha()]\r\n",
        "# \t# make lower case\r\n",
        "# \ttokens = [word.lower() for word in tokens]\r\n",
        "# \treturn tokens\r\n",
        "\r\n",
        "# cleaned_tokens = clean_doc(Poirot)\r\n",
        "\r\n",
        "\r\n",
        "# #cleaned_tokens = text_to_word_sequence(stripped)\r\n",
        "print(f'There are {len(cleaned_tokens)} words in the cleaned version of Poirot Investigates ({len(set(cleaned_tokens))} unique words).')\r\n",
        "print('------------------------------------------------------------------------------------------------------')\r\n",
        "print(f'Sample of 10 tokens: {cleaned_tokens[:10]}')"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 52809 words in the cleaned version of Poirot Investigates (6379 unique words).\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Sample of 10 tokens: ['the', 'adventure', 'of', 'the', 'western', 'star', 'i', 'was', 'standing', 'at']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI-4irhcDSMc",
        "outputId": "66318124-2819-4505-a231-68eb82254ad1"
      },
      "source": [
        "# organize into sequences of tokens\r\n",
        "length = 50 + 1\r\n",
        "sequences = list()\r\n",
        "for i in range(length, len(cleaned_tokens)):\r\n",
        "\t# select sequence of tokens\r\n",
        "\tseq = cleaned_tokens[i-length:i]\r\n",
        "\t# convert into a line\r\n",
        "\tline = ' '.join(seq)\r\n",
        "\t# store\r\n",
        "\tsequences.append(line)\r\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 52758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49gtHKnCec-J",
        "outputId": "4e2174f1-87b8-430d-814f-36af44a4d424"
      },
      "source": [
        "# txt = '\\n'.join(sequences)\r\n",
        "# txt_lines = txt.split('\\n')\r\n",
        "# defining tokenizer class to encode word seuences\r\n",
        "encoder = Tokenizer()\r\n",
        "encoder.fit_on_texts(sequences)\r\n",
        "encoded_lines = encoder.texts_to_sequences(sequences)\r\n",
        "print('--first line of text shown below in encoded form--')\r\n",
        "print(encoded_lines[0])"
      ],
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--first line of text shown below in encoded form--\n",
            "[1, 1590, 5, 1, 1113, 419, 4, 8, 656, 22, 1, 273, 5, 189, 821, 309, 50, 3086, 24, 1, 597, 1291, 216, 1290, 4, 1112, 256, 1111, 20, 728, 37, 13, 117, 140, 176, 15, 819, 33, 1, 6375, 5, 14, 3085, 401, 6373, 15, 33, 1, 501, 339, 86]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoI7rpqvqQQ_",
        "outputId": "4348ad03-17f1-45f7-8abb-7d8b77b47712"
      },
      "source": [
        "word_index = encoder.word_index\r\n",
        "print('printing 10 values from the `word_index` dictionary')\r\n",
        "print('---------------------------------------------------')\r\n",
        "dict(itertools.islice(word_index.items(), 10))"
      ],
      "execution_count": 364,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing 10 values from the `word_index` dictionary\n",
            "---------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 3,\n",
              " 'and': 6,\n",
              " 'he': 9,\n",
              " 'i': 4,\n",
              " 'in': 7,\n",
              " 'of': 5,\n",
              " 'the': 1,\n",
              " 'to': 2,\n",
              " 'was': 8,\n",
              " 'you': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 364
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYFh3GGkylPE"
      },
      "source": [
        "word_dim = len(set(cleaned_tokens))+1\r\n",
        "encoded_lines = np.array(encoded_lines)\r\n",
        "X, y = encoded_lines[:,:-1], encoded_lines[:,-1]\r\n",
        "y = to_categorical(y, num_classes=word_dim)\r\n",
        "sequence_len = X.shape[1]"
      ],
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQQkhq3J2PhF"
      },
      "source": [
        "**The model is defined**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nd8y-Al1jpq"
      },
      "source": [
        "# recurrent model"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5UxfeY11jnA"
      },
      "source": [
        "word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoF2aFAn0vJy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}