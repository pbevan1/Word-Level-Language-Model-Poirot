{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbevan1/Word-Level-Language-Model-Poirot/blob/main/poirot_recurrent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcNqUwL26kXy"
      },
      "source": [
        "**The raw text version of 'Poirot Investigates' by Agatha Christie is downloaded from Project Guttenberg**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aChca2dOaekw",
        "outputId": "94415ff7-cc43-4a19-9a2a-cb9c55b0e1c3"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.6/dist-packages (1.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.4.4)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (3.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgLS8cAuDLOI"
      },
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "import string\r\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import Model\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.layers import Embedding\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import itertools\r\n",
        "import re\r\n",
        "from pickle import dump\r\n",
        "\r\n",
        "from kerastuner.tuners import RandomSearch\r\n",
        "from kerastuner.tuners import Hyperband\r\n",
        "from kerastuner.engine.hyperparameters import HyperParameters"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR-66itlrWHj",
        "outputId": "697b9b61-38fe-4ea6-f6cb-7cf5ebd684a0"
      },
      "source": [
        "!curl -O https://www.gutenberg.org/files/61262/61262-0.txt"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  326k  100  326k    0     0  1527k      0 --:--:-- --:--:-- --:--:-- 1527k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx_8D2bi5I8f"
      },
      "source": [
        "**Unwanted text (publishing information etc) is deleted from the text and a new file saved as 'Poirot.txt'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95V3EoA4rcZY"
      },
      "source": [
        "#deleting unwanted text and front and back of book\r\n",
        "with open('61262-0.txt') as old, open('Poirot.txt', 'w') as new:\r\n",
        "    lines = old.readlines()\r\n",
        "    new.writelines(lines[110:-374])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7VAShBGx4i4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZzF1_Ob9-Vl",
        "outputId": "553fdb79-c715-4d19-ddd3-b10d53d7e36e"
      },
      "source": [
        "#opening the text file, saving into memory as `Poirot` and closing the file\r\n",
        "text = open('Poirot.txt', 'r')\r\n",
        "Poirot = text.read()\r\n",
        "text.close()\r\n",
        "\r\n",
        "#printing first 1000 characters to double check\r\n",
        "print(Poirot[:1000])"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  The Adventure of “The Western Star”\n",
            "\n",
            "I was standing at the window of Poirot’s rooms looking out idly on\n",
            "the street below.\n",
            "\n",
            "“That’s queer,” I ejaculated suddenly beneath my breath.\n",
            "\n",
            "“What is, _mon ami_?” asked Poirot placidly, from the depths of\n",
            "his comfortable chair.\n",
            "\n",
            "“Deduce, Poirot, from the following facts! Here is a young lady,\n",
            "richly dressed—fashionable hat, magnificent furs. She is coming\n",
            "along slowly, looking up at the houses as she goes. Unknown to her,\n",
            "she is being shadowed by three men and a middle-aged woman. They\n",
            "have just been joined by an errand boy who points after the girl,\n",
            "gesticulating as he does so. What drama is this being played? Is\n",
            "the girl a crook, and are the shadowers detectives preparing to\n",
            "arrest her? Or are _they_ the scoundrels, and are they plotting to\n",
            "attack an innocent victim? What does the great detective say?”\n",
            "\n",
            "“The great detective, _mon ami_, chooses, as ever, the simplest\n",
            "course. He rises to see for himself.” And my friend joined me at\n",
            "the window.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oBIRQiIjMbV",
        "outputId": "a1859168-bb3b-4d20-f6a7-6d322cd51707"
      },
      "source": [
        "# using keras' `text_to_word_sequence` function to tokenise the text\r\n",
        "# #additional characters added to filters to take out weird quotes\r\n",
        "cleaned_tokens = text_to_word_sequence(Poirot, filters='“”•!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\r\n",
        "\r\n",
        "print(f'There are {len(cleaned_tokens)} words in the cleaned version of Poirot Investigates ({len(set(cleaned_tokens))} unique words).')\r\n",
        "print('------------------------------------------------------------------------------------------------------')\r\n",
        "print(f'Sample of 10 tokens: {cleaned_tokens[:10]}')"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 52809 words in the cleaned version of Poirot Investigates (6379 unique words).\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Sample of 10 tokens: ['the', 'adventure', 'of', 'the', 'western', 'star', 'i', 'was', 'standing', 'at']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI-4irhcDSMc",
        "outputId": "c5c4e147-2ede-4281-90e8-40517065f6a9"
      },
      "source": [
        "#make sequences of 36 tokens\r\n",
        "length = 36\r\n",
        "sequences = list()\r\n",
        "for i in range(length, len(cleaned_tokens)):\r\n",
        "\t# select sequence of tokens\r\n",
        "\tseq = cleaned_tokens[i-length:i]\r\n",
        "\t# convert into a line\r\n",
        "\tline = ' '.join(seq)\r\n",
        "\t# store\r\n",
        "\tsequences.append(line)\r\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 52773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49gtHKnCec-J",
        "outputId": "5c801bd4-6f08-460c-cb7c-e936ae876133"
      },
      "source": [
        "# defining tokenizer class to encode word sequences\r\n",
        "encoder = Tokenizer()\r\n",
        "encoder.fit_on_texts(sequences)\r\n",
        "encoded_lines = encoder.texts_to_sequences(sequences)\r\n",
        "print('--first line of text shown below in encoded form--')\r\n",
        "print(encoded_lines[0])\r\n",
        "#saving encoder for use at time of generation\r\n",
        "dump(encoder, open('./encoder.pkl', 'wb'))"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--first line of text shown below in encoded form--\n",
            "[1, 1590, 5, 1, 1113, 419, 4, 8, 656, 22, 1, 273, 5, 189, 821, 309, 50, 3086, 24, 1, 597, 1291, 216, 1290, 4, 1112, 256, 1111, 20, 728, 37, 13, 117, 140, 176, 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoI7rpqvqQQ_",
        "outputId": "7f2cf709-f414-4f61-c1cb-ffc5433e6998"
      },
      "source": [
        "word_index = encoder.word_index\r\n",
        "print('printing 10 values from the `word_index` dictionary')\r\n",
        "print('---------------------------------------------------')\r\n",
        "dict(itertools.islice(word_index.items(), 10))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing 10 values from the `word_index` dictionary\n",
            "---------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 3,\n",
              " 'and': 6,\n",
              " 'he': 9,\n",
              " 'i': 4,\n",
              " 'in': 7,\n",
              " 'of': 5,\n",
              " 'the': 1,\n",
              " 'to': 2,\n",
              " 'was': 8,\n",
              " 'you': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYFh3GGkylPE"
      },
      "source": [
        "# defining numer of words in vocabulary\r\n",
        "word_dim = len(set(cleaned_tokens))+1\r\n",
        "# making encoded lines into numpy array so that model can process\r\n",
        "encoded_lines = np.array(encoded_lines)\r\n",
        "\r\n",
        "# defining predictor and response variables full dataset for full training\r\n",
        "X, y = encoded_lines[:,:-1], encoded_lines[:,-1]\r\n",
        "\r\n",
        "#splitting to train and validate for tuning\r\n",
        "X_train, y_train = encoded_lines[:45000,:-1], encoded_lines[:45000,-1]\r\n",
        "X_val, y_val = encoded_lines[45000:,:-1], encoded_lines[45000:,-1]\r\n",
        "\r\n",
        "# one hot encoding response variable to fit with softmax function dense layer\r\n",
        "y = to_categorical(y, num_classes=word_dim)\r\n",
        "y_train = to_categorical(y_train, num_classes=word_dim)\r\n",
        "y_val = to_categorical(y_val, num_classes=word_dim)\r\n",
        "\r\n",
        "# saving the sequence length for later use\r\n",
        "sequence_len = X.shape[1]\r\n",
        "sequence_len_train = X_train.shape[1]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQQkhq3J2PhF"
      },
      "source": [
        "**The model is defined**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nd8y-Al1jpq",
        "outputId": "b50a85d9-994d-4820-f16b-0a2c8b37b035"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52773, 35)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5UxfeY11jnA"
      },
      "source": [
        "# define tuner model\r\n",
        "\r\n",
        "def build_model(hp):\r\n",
        "  #sequential less flexible but using as it allows `predict_classes` later on\r\n",
        "  model = Sequential()\r\n",
        "\r\n",
        "  #adding embedding layer to project words into higher dimensional space\r\n",
        "  model.add(Embedding(word_dim, hp.Int('n_dimensions', 50, 200, 50), input_length=sequence_len))\r\n",
        "  \r\n",
        "  # tuning number of return sequence layers alongside number of units in each layer\r\n",
        "  for i in range(hp.Int('n_layers', 1, 3)):\r\n",
        "    model.add(LSTM(hp.Int('n_units', 64, 256, 64), return_sequences=True))\r\n",
        "\r\n",
        "  model.add(LSTM(hp.Int('n_units', 64, 256, 64)))\r\n",
        "\r\n",
        "  model.add(Dense(hp.Int('n_units', 64, 256, 64), activation='relu'))\r\n",
        "\r\n",
        "  #softmax dense layer to output probability distribution for each word\r\n",
        "  model.add(Dense(word_dim, activation='softmax'))\r\n",
        "\r\n",
        "  print(model.summary())\r\n",
        "\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5DYeWbUEoA0",
        "outputId": "5917798a-81c6-4fe0-ce2c-3266d146bdf2"
      },
      "source": [
        "# setting up tuner object and how long to tune for\r\n",
        "seed = 1 # seed to improve reproducability\r\n",
        "executions_per_trial = 2\r\n",
        "max_trials = 3\r\n",
        "tuner = RandomSearch(build_model, max_trials = max_trials , objective = 'val_accuracy', seed=seed, executions_per_trial = executions_per_trial, directory = '/content/drive/MyDrive/LOG_DIR_Poirot')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project /content/drive/MyDrive/LOG_DIR_Poirot/untitled_project/oracle.json\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 50)            319000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 30, 64)            29440     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6380)              414700    \n",
            "=================================================================\n",
            "Total params: 800,324\n",
            "Trainable params: 800,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "INFO:tensorflow:Reloading Tuner from /content/drive/MyDrive/LOG_DIR_Poirot/untitled_project/tuner0.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnOIhydEbTzv",
        "outputId": "dbe50638-7aa2-4648-e8be-ee9c89a42820"
      },
      "source": [
        "tuner.search_space_summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "n_dimensions (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 50, 'max_value': 200, 'step': 50, 'sampling': None}\n",
            "n_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': None}\n",
            "n_units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 64, 'sampling': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZUfhI6lEpTe",
        "outputId": "0522cf40-a1fb-494f-9543-2a3d181c5e9c"
      },
      "source": [
        "# executing search with validation data to evaluate\r\n",
        "tuner.search(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val,y_val))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 01m 49s]\n",
            "val_accuracy: 0.09318123757839203\n",
            "\n",
            "Best val_accuracy So Far: 0.09318123757839203\n",
            "Total elapsed time: 00h 05m 39s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INvZnWTQok2G"
      },
      "source": [
        "**The best model chosen by keras tuner is printed below. A smaller model with the least layers available in the search was chosen, and a mid value of 128 units per lstm layer. Since the data is very small, perhaps the deeper more complex models were quickly overfitting.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSpmXNU8cYln",
        "outputId": "b7eda3ca-9c25-43ff-9722-45398b90613d"
      },
      "source": [
        "#saving best model once tuning complete\r\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 6, 100)            638000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 6, 128)            117248    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6380)              823020    \n",
            "=================================================================\n",
            "Total params: 1,726,364\n",
            "Trainable params: 1,726,364\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyp21QTOrqCR"
      },
      "source": [
        "**The below is code from Keras for transfer learning of gloVe word embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsppBzO2r0ue"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "# !unzip -q glove.6B.zip\r\n",
        "\r\n",
        "\r\n",
        "path_to_glove_file = \"./glove.6B.100d.txt\"\r\n",
        "\r\n",
        "embeddings_index = {}\r\n",
        "with open(path_to_glove_file) as f:\r\n",
        "    for line in f:\r\n",
        "        word, coefs = line.split(maxsplit=1)\r\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\r\n",
        "        embeddings_index[word] = coefs\r\n",
        "\r\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\r\n",
        "\r\n",
        "\r\n",
        "num_tokens = len(set(cleaned_tokens)) + 2\r\n",
        "embedding_dim = 100\r\n",
        "hits = 0\r\n",
        "misses = 0\r\n",
        "\r\n",
        "# Prepare embedding matrix\r\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\r\n",
        "for word, i in word_index.items():\r\n",
        "    embedding_vector = embeddings_index.get(word)\r\n",
        "    if embedding_vector is not None:\r\n",
        "        # Words not found in embedding index will be all-zeros.\r\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\r\n",
        "        embedding_matrix[i] = embedding_vector\r\n",
        "        hits += 1\r\n",
        "    else:\r\n",
        "        misses += 1\r\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\r\n",
        "\r\n",
        "from keras.initializers import  Constant\r\n",
        "\r\n",
        "embedding_layer = Embedding(\r\n",
        "    num_tokens,\r\n",
        "    embedding_dim,\r\n",
        "    embeddings_initializer=Constant(embedding_matrix),\r\n",
        "    trainable=False,\r\n",
        ")"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMbs2T4_ivmZ"
      },
      "source": [
        "**The chosen model is defined below and trained on the full book**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3OUL1LUpaUy",
        "outputId": "334aa414-824e-4dfc-cd34-f2f741b1ce56"
      },
      "source": [
        "input_layer = Input(shape=(sequence_len,))\r\n",
        "\r\n",
        "#adding embedding layer to project words into higher dimensional space\r\n",
        "embedded = embedding_layer(input_layer)\r\n",
        "\r\n",
        "lstm1 = LSTM(128, return_sequences=True)(embedded)\r\n",
        "\r\n",
        "lstm2 = LSTM(128, return_sequences=True)(lstm1)\r\n",
        "\r\n",
        "lstm3 = LSTM(128)(lstm2)\r\n",
        "\r\n",
        "dense = Dense(128, activation='relu')(lstm3)\r\n",
        "\r\n",
        "#softmax dense layer to output probability distribution for each word\r\n",
        "softmax = Dense(word_dim, activation='softmax')(dense)\r\n",
        "\r\n",
        "model = Model(inputs=input_layer, outputs=softmax)\r\n",
        "\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 35)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      multiple                  638100    \n",
            "_________________________________________________________________\n",
            "lstm_20 (LSTM)               (None, 35, 128)           117248    \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 35, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 6380)              823020    \n",
            "=================================================================\n",
            "Total params: 1,858,048\n",
            "Trainable params: 1,219,948\n",
            "Non-trainable params: 638,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n9wakF81b0q"
      },
      "source": [
        "**The model is now trained with the full data for 50 epochs. 50 Epochs would likely overfit when evaluated with the validation set used previously, but since the model is not being evaluated on test data and accuracy, rather plausability, this doesn't matter so much and experimentation showed longer training to give more plausible results in the style of agatha christie.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3885cBkRwDLc",
        "outputId": "b303cf8f-9794-4333-ee2c-2bfb83f493b7"
      },
      "source": [
        "model.fit(X, y, batch_size=64, epochs=500)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 2.2324 - accuracy: 0.4465\n",
            "Epoch 2/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 2.1240 - accuracy: 0.4712\n",
            "Epoch 3/500\n",
            "825/825 [==============================] - 9s 12ms/step - loss: 2.0427 - accuracy: 0.4870\n",
            "Epoch 4/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 2.0062 - accuracy: 0.4946\n",
            "Epoch 5/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 2.0895 - accuracy: 0.4765\n",
            "Epoch 6/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 2.1800 - accuracy: 0.4566\n",
            "Epoch 7/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 2.0025 - accuracy: 0.4977\n",
            "Epoch 8/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.9700 - accuracy: 0.5028\n",
            "Epoch 9/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.9398 - accuracy: 0.5083\n",
            "Epoch 10/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 2.2685 - accuracy: 0.4441\n",
            "Epoch 11/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 2.0637 - accuracy: 0.4723\n",
            "Epoch 12/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.8700 - accuracy: 0.5187\n",
            "Epoch 13/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.7995 - accuracy: 0.5382\n",
            "Epoch 14/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.7648 - accuracy: 0.5462\n",
            "Epoch 15/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.7277 - accuracy: 0.5533\n",
            "Epoch 16/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.7190 - accuracy: 0.5556\n",
            "Epoch 17/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.7123 - accuracy: 0.5544\n",
            "Epoch 18/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.6804 - accuracy: 0.5605\n",
            "Epoch 19/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.6728 - accuracy: 0.5631\n",
            "Epoch 20/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.6693 - accuracy: 0.5632\n",
            "Epoch 21/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.6342 - accuracy: 0.5717\n",
            "Epoch 22/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.6086 - accuracy: 0.5772\n",
            "Epoch 23/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.5934 - accuracy: 0.5810\n",
            "Epoch 24/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.5768 - accuracy: 0.5828\n",
            "Epoch 25/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.5518 - accuracy: 0.5901\n",
            "Epoch 26/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.5166 - accuracy: 0.5989\n",
            "Epoch 27/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.5155 - accuracy: 0.6007\n",
            "Epoch 28/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.5231 - accuracy: 0.5977\n",
            "Epoch 29/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.4687 - accuracy: 0.6094\n",
            "Epoch 30/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.4552 - accuracy: 0.6161\n",
            "Epoch 31/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 1.4289 - accuracy: 0.6215\n",
            "Epoch 32/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.3951 - accuracy: 0.6275\n",
            "Epoch 33/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.3903 - accuracy: 0.6297\n",
            "Epoch 34/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.4262 - accuracy: 0.6205\n",
            "Epoch 35/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.4875 - accuracy: 0.6080\n",
            "Epoch 36/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.3789 - accuracy: 0.6359\n",
            "Epoch 37/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.4261 - accuracy: 0.6224\n",
            "Epoch 38/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 1.3809 - accuracy: 0.6347\n",
            "Epoch 39/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.3487 - accuracy: 0.6410\n",
            "Epoch 40/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.3431 - accuracy: 0.6418\n",
            "Epoch 41/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 1.3353 - accuracy: 0.6436\n",
            "Epoch 42/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.2953 - accuracy: 0.6521\n",
            "Epoch 43/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 1.3401 - accuracy: 0.6433\n",
            "Epoch 44/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.3306 - accuracy: 0.6455\n",
            "Epoch 45/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.4472 - accuracy: 0.6282\n",
            "Epoch 46/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.4844 - accuracy: 0.6125\n",
            "Epoch 47/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 1.3279 - accuracy: 0.6539\n",
            "Epoch 48/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.2656 - accuracy: 0.6673\n",
            "Epoch 49/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.2826 - accuracy: 0.6602\n",
            "Epoch 50/500\n",
            "825/825 [==============================] - 9s 12ms/step - loss: 1.2926 - accuracy: 0.6566\n",
            "Epoch 51/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.3667 - accuracy: 0.6413\n",
            "Epoch 52/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 1.3216 - accuracy: 0.6502\n",
            "Epoch 53/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.6695 - accuracy: 0.5888\n",
            "Epoch 54/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.5461 - accuracy: 0.6003\n",
            "Epoch 55/500\n",
            "825/825 [==============================] - 9s 12ms/step - loss: 1.6652 - accuracy: 0.5705\n",
            "Epoch 56/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.4765 - accuracy: 0.6139\n",
            "Epoch 57/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.2695 - accuracy: 0.6650\n",
            "Epoch 58/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.5326 - accuracy: 0.6053\n",
            "Epoch 59/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.5305 - accuracy: 0.5976\n",
            "Epoch 60/500\n",
            "825/825 [==============================] - 9s 12ms/step - loss: 1.2925 - accuracy: 0.6578\n",
            "Epoch 61/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.2092 - accuracy: 0.6791\n",
            "Epoch 62/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.1924 - accuracy: 0.6816\n",
            "Epoch 63/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.1898 - accuracy: 0.6808\n",
            "Epoch 64/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.1905 - accuracy: 0.6793\n",
            "Epoch 65/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.1649 - accuracy: 0.6827\n",
            "Epoch 66/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 1.1672 - accuracy: 0.6803\n",
            "Epoch 67/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.1597 - accuracy: 0.6822\n",
            "Epoch 68/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.1342 - accuracy: 0.6895\n",
            "Epoch 69/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.1331 - accuracy: 0.6883\n",
            "Epoch 70/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.1114 - accuracy: 0.6941\n",
            "Epoch 71/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.0834 - accuracy: 0.7003\n",
            "Epoch 72/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.0721 - accuracy: 0.7044\n",
            "Epoch 73/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.0561 - accuracy: 0.7060\n",
            "Epoch 74/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.0443 - accuracy: 0.7086\n",
            "Epoch 75/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.0146 - accuracy: 0.7166\n",
            "Epoch 76/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.9941 - accuracy: 0.7235\n",
            "Epoch 77/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 1.0352 - accuracy: 0.7105\n",
            "Epoch 78/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.9340 - accuracy: 0.7397\n",
            "Epoch 79/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.9396 - accuracy: 0.7364\n",
            "Epoch 80/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.9495 - accuracy: 0.7318\n",
            "Epoch 81/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.9487 - accuracy: 0.7323\n",
            "Epoch 82/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.9249 - accuracy: 0.7424\n",
            "Epoch 83/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.8903 - accuracy: 0.7502\n",
            "Epoch 84/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.8945 - accuracy: 0.7484\n",
            "Epoch 85/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.8778 - accuracy: 0.7530\n",
            "Epoch 86/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.8511 - accuracy: 0.7577\n",
            "Epoch 87/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.8473 - accuracy: 0.7590\n",
            "Epoch 88/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.8317 - accuracy: 0.7634\n",
            "Epoch 89/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.8461 - accuracy: 0.7601\n",
            "Epoch 90/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.8212 - accuracy: 0.7656\n",
            "Epoch 91/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.7854 - accuracy: 0.7766\n",
            "Epoch 92/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.7691 - accuracy: 0.7814\n",
            "Epoch 93/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.7775 - accuracy: 0.7790\n",
            "Epoch 94/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.7760 - accuracy: 0.7784\n",
            "Epoch 95/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.7650 - accuracy: 0.7816\n",
            "Epoch 96/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.7191 - accuracy: 0.7946\n",
            "Epoch 97/500\n",
            "825/825 [==============================] - 9s 12ms/step - loss: 0.7278 - accuracy: 0.7921\n",
            "Epoch 98/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.7464 - accuracy: 0.7855\n",
            "Epoch 99/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.7111 - accuracy: 0.7969\n",
            "Epoch 100/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.6738 - accuracy: 0.8093\n",
            "Epoch 101/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.6877 - accuracy: 0.8017\n",
            "Epoch 102/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.7073 - accuracy: 0.7953\n",
            "Epoch 103/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.6674 - accuracy: 0.8069\n",
            "Epoch 104/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.6470 - accuracy: 0.8133\n",
            "Epoch 105/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.6544 - accuracy: 0.8123\n",
            "Epoch 106/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.6351 - accuracy: 0.8175\n",
            "Epoch 107/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.6254 - accuracy: 0.8166\n",
            "Epoch 108/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.6332 - accuracy: 0.8167\n",
            "Epoch 109/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.6225 - accuracy: 0.8196\n",
            "Epoch 110/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 0.6050 - accuracy: 0.8240\n",
            "Epoch 111/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5674 - accuracy: 0.8359\n",
            "Epoch 112/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5784 - accuracy: 0.8331\n",
            "Epoch 113/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5971 - accuracy: 0.8250\n",
            "Epoch 114/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5865 - accuracy: 0.8299\n",
            "Epoch 115/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5621 - accuracy: 0.8365\n",
            "Epoch 116/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5607 - accuracy: 0.8386\n",
            "Epoch 117/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5630 - accuracy: 0.8370\n",
            "Epoch 118/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5249 - accuracy: 0.8477\n",
            "Epoch 119/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5490 - accuracy: 0.8395\n",
            "Epoch 120/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5429 - accuracy: 0.8408\n",
            "Epoch 121/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5116 - accuracy: 0.8482\n",
            "Epoch 122/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5073 - accuracy: 0.8510\n",
            "Epoch 123/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5032 - accuracy: 0.8531\n",
            "Epoch 124/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4991 - accuracy: 0.8542\n",
            "Epoch 125/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4885 - accuracy: 0.8576\n",
            "Epoch 126/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 0.4685 - accuracy: 0.8628\n",
            "Epoch 127/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.5045 - accuracy: 0.8510\n",
            "Epoch 128/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.4973 - accuracy: 0.8524\n",
            "Epoch 129/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4607 - accuracy: 0.8643\n",
            "Epoch 130/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.4704 - accuracy: 0.8620\n",
            "Epoch 131/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4656 - accuracy: 0.8637\n",
            "Epoch 132/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4408 - accuracy: 0.8710\n",
            "Epoch 133/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4527 - accuracy: 0.8663\n",
            "Epoch 134/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 0.4586 - accuracy: 0.8656\n",
            "Epoch 135/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4594 - accuracy: 0.8655\n",
            "Epoch 136/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4415 - accuracy: 0.8685\n",
            "Epoch 137/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4291 - accuracy: 0.8745\n",
            "Epoch 138/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3980 - accuracy: 0.8823\n",
            "Epoch 139/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4185 - accuracy: 0.8779\n",
            "Epoch 140/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4357 - accuracy: 0.8723\n",
            "Epoch 141/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3899 - accuracy: 0.8864\n",
            "Epoch 142/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4178 - accuracy: 0.8762\n",
            "Epoch 143/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3946 - accuracy: 0.8824\n",
            "Epoch 144/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4000 - accuracy: 0.8824\n",
            "Epoch 145/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3999 - accuracy: 0.8812\n",
            "Epoch 146/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3669 - accuracy: 0.8935\n",
            "Epoch 147/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3866 - accuracy: 0.8859\n",
            "Epoch 148/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3979 - accuracy: 0.8808\n",
            "Epoch 149/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3671 - accuracy: 0.8903\n",
            "Epoch 150/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3770 - accuracy: 0.8876\n",
            "Epoch 151/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3493 - accuracy: 0.8951\n",
            "Epoch 152/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3399 - accuracy: 0.9012\n",
            "Epoch 153/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.4409 - accuracy: 0.8678\n",
            "Epoch 154/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3651 - accuracy: 0.8917\n",
            "Epoch 155/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3286 - accuracy: 0.9049\n",
            "Epoch 156/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3601 - accuracy: 0.8927\n",
            "Epoch 157/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3611 - accuracy: 0.8925\n",
            "Epoch 158/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3293 - accuracy: 0.9021\n",
            "Epoch 159/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3573 - accuracy: 0.8937\n",
            "Epoch 160/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3397 - accuracy: 0.8982\n",
            "Epoch 161/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3351 - accuracy: 0.9005\n",
            "Epoch 162/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3680 - accuracy: 0.8902\n",
            "Epoch 163/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3285 - accuracy: 0.9016\n",
            "Epoch 164/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2870 - accuracy: 0.9154\n",
            "Epoch 165/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3382 - accuracy: 0.8995\n",
            "Epoch 166/500\n",
            "825/825 [==============================] - 9s 11ms/step - loss: 0.3803 - accuracy: 0.8866\n",
            "Epoch 167/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3191 - accuracy: 0.9042\n",
            "Epoch 168/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2811 - accuracy: 0.9182\n",
            "Epoch 169/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3576 - accuracy: 0.8941\n",
            "Epoch 170/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3117 - accuracy: 0.9096\n",
            "Epoch 171/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3015 - accuracy: 0.9117\n",
            "Epoch 172/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3057 - accuracy: 0.9088\n",
            "Epoch 173/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2950 - accuracy: 0.9124\n",
            "Epoch 174/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3476 - accuracy: 0.8964\n",
            "Epoch 175/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3151 - accuracy: 0.9045\n",
            "Epoch 176/500\n",
            "825/825 [==============================] - 9s 12ms/step - loss: 0.2823 - accuracy: 0.9173\n",
            "Epoch 177/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2742 - accuracy: 0.9204\n",
            "Epoch 178/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3275 - accuracy: 0.9012\n",
            "Epoch 179/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2888 - accuracy: 0.9163\n",
            "Epoch 180/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3210 - accuracy: 0.9048\n",
            "Epoch 181/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3087 - accuracy: 0.9083\n",
            "Epoch 182/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2878 - accuracy: 0.9148\n",
            "Epoch 183/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2834 - accuracy: 0.9174\n",
            "Epoch 184/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3066 - accuracy: 0.9103\n",
            "Epoch 185/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2753 - accuracy: 0.9184\n",
            "Epoch 186/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2696 - accuracy: 0.9196\n",
            "Epoch 187/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3128 - accuracy: 0.9067\n",
            "Epoch 188/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2823 - accuracy: 0.9163\n",
            "Epoch 189/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2698 - accuracy: 0.9203\n",
            "Epoch 190/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2231 - accuracy: 0.9359\n",
            "Epoch 191/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3650 - accuracy: 0.8918\n",
            "Epoch 192/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2692 - accuracy: 0.9191\n",
            "Epoch 193/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2358 - accuracy: 0.9321\n",
            "Epoch 194/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2755 - accuracy: 0.9197\n",
            "Epoch 195/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.3136 - accuracy: 0.9076\n",
            "Epoch 196/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2209 - accuracy: 0.9353\n",
            "Epoch 197/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2770 - accuracy: 0.9180\n",
            "Epoch 198/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2960 - accuracy: 0.9105\n",
            "Epoch 199/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2706 - accuracy: 0.9194\n",
            "Epoch 200/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2594 - accuracy: 0.9239\n",
            "Epoch 201/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2373 - accuracy: 0.9303\n",
            "Epoch 202/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2739 - accuracy: 0.9194\n",
            "Epoch 203/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2798 - accuracy: 0.9169\n",
            "Epoch 204/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.3123 - accuracy: 0.9099\n",
            "Epoch 205/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2280 - accuracy: 0.9322\n",
            "Epoch 206/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2005 - accuracy: 0.9415\n",
            "Epoch 207/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2054 - accuracy: 0.9387\n",
            "Epoch 208/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2282 - accuracy: 0.9318\n",
            "Epoch 209/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2836 - accuracy: 0.9153\n",
            "Epoch 210/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2622 - accuracy: 0.9207\n",
            "Epoch 211/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2372 - accuracy: 0.9299\n",
            "Epoch 212/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2481 - accuracy: 0.9249\n",
            "Epoch 213/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2606 - accuracy: 0.9218\n",
            "Epoch 214/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2875 - accuracy: 0.9151\n",
            "Epoch 215/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2570 - accuracy: 0.9242\n",
            "Epoch 216/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2164 - accuracy: 0.9356\n",
            "Epoch 217/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2407 - accuracy: 0.9287\n",
            "Epoch 218/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2688 - accuracy: 0.9197\n",
            "Epoch 219/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2541 - accuracy: 0.9249\n",
            "Epoch 220/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2560 - accuracy: 0.9242\n",
            "Epoch 221/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2321 - accuracy: 0.9312\n",
            "Epoch 222/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2250 - accuracy: 0.9322\n",
            "Epoch 223/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2947 - accuracy: 0.9135\n",
            "Epoch 224/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2377 - accuracy: 0.9283\n",
            "Epoch 225/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2287 - accuracy: 0.9318\n",
            "Epoch 226/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2383 - accuracy: 0.9292\n",
            "Epoch 227/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2406 - accuracy: 0.9270\n",
            "Epoch 228/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2202 - accuracy: 0.9362\n",
            "Epoch 229/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2787 - accuracy: 0.9161\n",
            "Epoch 230/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2110 - accuracy: 0.9362\n",
            "Epoch 231/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2416 - accuracy: 0.9281\n",
            "Epoch 232/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2500 - accuracy: 0.9243\n",
            "Epoch 233/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2421 - accuracy: 0.9279\n",
            "Epoch 234/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2394 - accuracy: 0.9284\n",
            "Epoch 235/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2599 - accuracy: 0.9244\n",
            "Epoch 236/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2322 - accuracy: 0.9302\n",
            "Epoch 237/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2086 - accuracy: 0.9386\n",
            "Epoch 238/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2424 - accuracy: 0.9270\n",
            "Epoch 239/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1999 - accuracy: 0.9418\n",
            "Epoch 240/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2515 - accuracy: 0.9249\n",
            "Epoch 241/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2544 - accuracy: 0.9248\n",
            "Epoch 242/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2075 - accuracy: 0.9391\n",
            "Epoch 243/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2684 - accuracy: 0.9211\n",
            "Epoch 244/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2260 - accuracy: 0.9331\n",
            "Epoch 245/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2135 - accuracy: 0.9369\n",
            "Epoch 246/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2405 - accuracy: 0.9300\n",
            "Epoch 247/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2022 - accuracy: 0.9394\n",
            "Epoch 248/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2426 - accuracy: 0.9285\n",
            "Epoch 249/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2583 - accuracy: 0.9233\n",
            "Epoch 250/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1931 - accuracy: 0.9433\n",
            "Epoch 251/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2388 - accuracy: 0.9283\n",
            "Epoch 252/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2198 - accuracy: 0.9343\n",
            "Epoch 253/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2408 - accuracy: 0.9283\n",
            "Epoch 254/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2199 - accuracy: 0.9352\n",
            "Epoch 255/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2033 - accuracy: 0.9395\n",
            "Epoch 256/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2403 - accuracy: 0.9288\n",
            "Epoch 257/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2019 - accuracy: 0.9401\n",
            "Epoch 258/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2427 - accuracy: 0.9299\n",
            "Epoch 259/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2596 - accuracy: 0.9234\n",
            "Epoch 260/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2055 - accuracy: 0.9388\n",
            "Epoch 261/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1971 - accuracy: 0.9408\n",
            "Epoch 262/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2466 - accuracy: 0.9258\n",
            "Epoch 263/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2069 - accuracy: 0.9403\n",
            "Epoch 264/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1843 - accuracy: 0.9455\n",
            "Epoch 265/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2326 - accuracy: 0.9311\n",
            "Epoch 266/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2306 - accuracy: 0.9320\n",
            "Epoch 267/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2000 - accuracy: 0.9410\n",
            "Epoch 268/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2363 - accuracy: 0.9307\n",
            "Epoch 269/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2109 - accuracy: 0.9375\n",
            "Epoch 270/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1997 - accuracy: 0.9398\n",
            "Epoch 271/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2016 - accuracy: 0.9407\n",
            "Epoch 272/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2314 - accuracy: 0.9326\n",
            "Epoch 273/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2368 - accuracy: 0.9317\n",
            "Epoch 274/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2084 - accuracy: 0.9378\n",
            "Epoch 275/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2274 - accuracy: 0.9324\n",
            "Epoch 276/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1905 - accuracy: 0.9427\n",
            "Epoch 277/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1981 - accuracy: 0.9403\n",
            "Epoch 278/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2386 - accuracy: 0.9286\n",
            "Epoch 279/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2199 - accuracy: 0.9347\n",
            "Epoch 280/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2042 - accuracy: 0.9389\n",
            "Epoch 281/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2014 - accuracy: 0.9399\n",
            "Epoch 282/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2185 - accuracy: 0.9361\n",
            "Epoch 283/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1893 - accuracy: 0.9430\n",
            "Epoch 284/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1787 - accuracy: 0.9464\n",
            "Epoch 285/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2451 - accuracy: 0.9283\n",
            "Epoch 286/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2158 - accuracy: 0.9356\n",
            "Epoch 287/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1956 - accuracy: 0.9421\n",
            "Epoch 288/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2099 - accuracy: 0.9405\n",
            "Epoch 289/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2293 - accuracy: 0.9336\n",
            "Epoch 290/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2087 - accuracy: 0.9378\n",
            "Epoch 291/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1943 - accuracy: 0.9429\n",
            "Epoch 292/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2074 - accuracy: 0.9385\n",
            "Epoch 293/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2045 - accuracy: 0.9389\n",
            "Epoch 294/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2171 - accuracy: 0.9361\n",
            "Epoch 295/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2177 - accuracy: 0.9360\n",
            "Epoch 296/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1992 - accuracy: 0.9416\n",
            "Epoch 297/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2183 - accuracy: 0.9359\n",
            "Epoch 298/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1699 - accuracy: 0.9491\n",
            "Epoch 299/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1907 - accuracy: 0.9430\n",
            "Epoch 300/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2352 - accuracy: 0.9305\n",
            "Epoch 301/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1919 - accuracy: 0.9431\n",
            "Epoch 302/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2102 - accuracy: 0.9408\n",
            "Epoch 303/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1993 - accuracy: 0.9401\n",
            "Epoch 304/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2047 - accuracy: 0.9401\n",
            "Epoch 305/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1954 - accuracy: 0.9430\n",
            "Epoch 306/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1964 - accuracy: 0.9412\n",
            "Epoch 307/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2406 - accuracy: 0.9284\n",
            "Epoch 308/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1610 - accuracy: 0.9526\n",
            "Epoch 309/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2231 - accuracy: 0.9351\n",
            "Epoch 310/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.2316 - accuracy: 0.9293\n",
            "Epoch 311/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1578 - accuracy: 0.9539\n",
            "Epoch 312/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2021 - accuracy: 0.9399\n",
            "Epoch 313/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2178 - accuracy: 0.9368\n",
            "Epoch 314/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1912 - accuracy: 0.9432\n",
            "Epoch 315/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2140 - accuracy: 0.9365\n",
            "Epoch 316/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1954 - accuracy: 0.9413\n",
            "Epoch 317/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1781 - accuracy: 0.9479\n",
            "Epoch 318/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2178 - accuracy: 0.9362\n",
            "Epoch 319/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1716 - accuracy: 0.9496\n",
            "Epoch 320/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2127 - accuracy: 0.9382\n",
            "Epoch 321/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2473 - accuracy: 0.9277\n",
            "Epoch 322/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1504 - accuracy: 0.9555\n",
            "Epoch 323/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1913 - accuracy: 0.9439\n",
            "Epoch 324/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2006 - accuracy: 0.9407\n",
            "Epoch 325/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1996 - accuracy: 0.9399\n",
            "Epoch 326/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1666 - accuracy: 0.9517\n",
            "Epoch 327/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2373 - accuracy: 0.9305\n",
            "Epoch 328/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1925 - accuracy: 0.9439\n",
            "Epoch 329/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1769 - accuracy: 0.9483\n",
            "Epoch 330/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2061 - accuracy: 0.9398\n",
            "Epoch 331/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2120 - accuracy: 0.9384\n",
            "Epoch 332/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1888 - accuracy: 0.9451\n",
            "Epoch 333/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1736 - accuracy: 0.9490\n",
            "Epoch 334/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2178 - accuracy: 0.9367\n",
            "Epoch 335/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2081 - accuracy: 0.9390\n",
            "Epoch 336/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1775 - accuracy: 0.9490\n",
            "Epoch 337/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1989 - accuracy: 0.9409\n",
            "Epoch 338/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2145 - accuracy: 0.9365\n",
            "Epoch 339/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1876 - accuracy: 0.9452\n",
            "Epoch 340/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1796 - accuracy: 0.9465\n",
            "Epoch 341/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1837 - accuracy: 0.9462\n",
            "Epoch 342/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2070 - accuracy: 0.9386\n",
            "Epoch 343/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1882 - accuracy: 0.9447\n",
            "Epoch 344/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1807 - accuracy: 0.9453\n",
            "Epoch 345/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2076 - accuracy: 0.9393\n",
            "Epoch 346/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1896 - accuracy: 0.9429\n",
            "Epoch 347/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1945 - accuracy: 0.9424\n",
            "Epoch 348/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1672 - accuracy: 0.9503\n",
            "Epoch 349/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1990 - accuracy: 0.9423\n",
            "Epoch 350/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2060 - accuracy: 0.9405\n",
            "Epoch 351/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1696 - accuracy: 0.9505\n",
            "Epoch 352/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1743 - accuracy: 0.9486\n",
            "Epoch 353/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2108 - accuracy: 0.9380\n",
            "Epoch 354/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1979 - accuracy: 0.9426\n",
            "Epoch 355/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2058 - accuracy: 0.9409\n",
            "Epoch 356/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1922 - accuracy: 0.9446\n",
            "Epoch 357/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1633 - accuracy: 0.9517\n",
            "Epoch 358/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2079 - accuracy: 0.9394\n",
            "Epoch 359/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2262 - accuracy: 0.9353\n",
            "Epoch 360/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1529 - accuracy: 0.9547\n",
            "Epoch 361/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1376 - accuracy: 0.9588\n",
            "Epoch 362/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2100 - accuracy: 0.9382\n",
            "Epoch 363/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2069 - accuracy: 0.9408\n",
            "Epoch 364/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1663 - accuracy: 0.9497\n",
            "Epoch 365/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2248 - accuracy: 0.9352\n",
            "Epoch 366/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2000 - accuracy: 0.9427\n",
            "Epoch 367/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1847 - accuracy: 0.9449\n",
            "Epoch 368/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1720 - accuracy: 0.9492\n",
            "Epoch 369/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1740 - accuracy: 0.9488\n",
            "Epoch 370/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.2037 - accuracy: 0.9412\n",
            "Epoch 371/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2041 - accuracy: 0.9423\n",
            "Epoch 372/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2088 - accuracy: 0.9392\n",
            "Epoch 373/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1524 - accuracy: 0.9559\n",
            "Epoch 374/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1715 - accuracy: 0.9493\n",
            "Epoch 375/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2081 - accuracy: 0.9385\n",
            "Epoch 376/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1773 - accuracy: 0.9478\n",
            "Epoch 377/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1834 - accuracy: 0.9459\n",
            "Epoch 378/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2123 - accuracy: 0.9407\n",
            "Epoch 379/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1697 - accuracy: 0.9506\n",
            "Epoch 380/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1636 - accuracy: 0.9518\n",
            "Epoch 381/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2261 - accuracy: 0.9351\n",
            "Epoch 382/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1922 - accuracy: 0.9443\n",
            "Epoch 383/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1553 - accuracy: 0.9548\n",
            "Epoch 384/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1845 - accuracy: 0.9470\n",
            "Epoch 385/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2101 - accuracy: 0.9385\n",
            "Epoch 386/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1723 - accuracy: 0.9495\n",
            "Epoch 387/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.2211 - accuracy: 0.9379\n",
            "Epoch 388/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1703 - accuracy: 0.9500\n",
            "Epoch 389/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1450 - accuracy: 0.9573\n",
            "Epoch 390/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1866 - accuracy: 0.9472\n",
            "Epoch 391/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2183 - accuracy: 0.9377\n",
            "Epoch 392/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1596 - accuracy: 0.9528\n",
            "Epoch 393/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1441 - accuracy: 0.9581\n",
            "Epoch 394/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2264 - accuracy: 0.9332\n",
            "Epoch 395/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1824 - accuracy: 0.9470\n",
            "Epoch 396/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1281 - accuracy: 0.9627\n",
            "Epoch 397/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1950 - accuracy: 0.9424\n",
            "Epoch 398/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1793 - accuracy: 0.9478\n",
            "Epoch 399/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1828 - accuracy: 0.9472\n",
            "Epoch 400/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1660 - accuracy: 0.9517\n",
            "Epoch 401/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1909 - accuracy: 0.9445\n",
            "Epoch 402/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1577 - accuracy: 0.9531\n",
            "Epoch 403/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1908 - accuracy: 0.9438\n",
            "Epoch 404/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2196 - accuracy: 0.9375\n",
            "Epoch 405/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1645 - accuracy: 0.9506\n",
            "Epoch 406/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1426 - accuracy: 0.9573\n",
            "Epoch 407/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2121 - accuracy: 0.9391\n",
            "Epoch 408/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1852 - accuracy: 0.9466\n",
            "Epoch 409/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1748 - accuracy: 0.9489\n",
            "Epoch 410/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1881 - accuracy: 0.9450\n",
            "Epoch 411/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1849 - accuracy: 0.9459\n",
            "Epoch 412/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1712 - accuracy: 0.9505\n",
            "Epoch 413/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1899 - accuracy: 0.9444\n",
            "Epoch 414/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2158 - accuracy: 0.9389\n",
            "Epoch 415/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1914 - accuracy: 0.9460\n",
            "Epoch 416/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1535 - accuracy: 0.9553\n",
            "Epoch 417/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1739 - accuracy: 0.9496\n",
            "Epoch 418/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1967 - accuracy: 0.9421\n",
            "Epoch 419/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1431 - accuracy: 0.9575\n",
            "Epoch 420/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.2086 - accuracy: 0.9392\n",
            "Epoch 421/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1757 - accuracy: 0.9489\n",
            "Epoch 422/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1909 - accuracy: 0.9452\n",
            "Epoch 423/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1633 - accuracy: 0.9514\n",
            "Epoch 424/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1761 - accuracy: 0.9493\n",
            "Epoch 425/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1848 - accuracy: 0.9458\n",
            "Epoch 426/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.2088 - accuracy: 0.9410\n",
            "Epoch 427/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1737 - accuracy: 0.9481\n",
            "Epoch 428/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1598 - accuracy: 0.9525\n",
            "Epoch 429/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2076 - accuracy: 0.9400\n",
            "Epoch 430/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1617 - accuracy: 0.9514\n",
            "Epoch 431/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1389 - accuracy: 0.9596\n",
            "Epoch 432/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2153 - accuracy: 0.9377\n",
            "Epoch 433/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1925 - accuracy: 0.9443\n",
            "Epoch 434/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1557 - accuracy: 0.9540\n",
            "Epoch 435/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1892 - accuracy: 0.9454\n",
            "Epoch 436/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1810 - accuracy: 0.9480\n",
            "Epoch 437/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1970 - accuracy: 0.9435\n",
            "Epoch 438/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1693 - accuracy: 0.9516\n",
            "Epoch 439/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1697 - accuracy: 0.9495\n",
            "Epoch 440/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1749 - accuracy: 0.9482\n",
            "Epoch 441/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1914 - accuracy: 0.9462\n",
            "Epoch 442/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1412 - accuracy: 0.9594\n",
            "Epoch 443/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1759 - accuracy: 0.9498\n",
            "Epoch 444/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1781 - accuracy: 0.9482\n",
            "Epoch 445/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1974 - accuracy: 0.9449\n",
            "Epoch 446/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1553 - accuracy: 0.9538\n",
            "Epoch 447/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1856 - accuracy: 0.9464\n",
            "Epoch 448/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1825 - accuracy: 0.9468\n",
            "Epoch 449/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1657 - accuracy: 0.9510\n",
            "Epoch 450/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1459 - accuracy: 0.9565\n",
            "Epoch 451/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1908 - accuracy: 0.9454\n",
            "Epoch 452/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1921 - accuracy: 0.9451\n",
            "Epoch 453/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1787 - accuracy: 0.9491\n",
            "Epoch 454/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1713 - accuracy: 0.9522\n",
            "Epoch 455/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1971 - accuracy: 0.9424\n",
            "Epoch 456/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1579 - accuracy: 0.9541\n",
            "Epoch 457/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1659 - accuracy: 0.9519\n",
            "Epoch 458/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1438 - accuracy: 0.9580\n",
            "Epoch 459/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1810 - accuracy: 0.9485\n",
            "Epoch 460/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2041 - accuracy: 0.9403\n",
            "Epoch 461/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1483 - accuracy: 0.9557\n",
            "Epoch 462/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1770 - accuracy: 0.9491\n",
            "Epoch 463/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.2015 - accuracy: 0.9415\n",
            "Epoch 464/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1794 - accuracy: 0.9472\n",
            "Epoch 465/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1685 - accuracy: 0.9517\n",
            "Epoch 466/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1685 - accuracy: 0.9513\n",
            "Epoch 467/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.2009 - accuracy: 0.9439\n",
            "Epoch 468/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1682 - accuracy: 0.9512\n",
            "Epoch 469/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1596 - accuracy: 0.9543\n",
            "Epoch 470/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1908 - accuracy: 0.9456\n",
            "Epoch 471/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1727 - accuracy: 0.9488\n",
            "Epoch 472/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1408 - accuracy: 0.9598\n",
            "Epoch 473/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1988 - accuracy: 0.9443\n",
            "Epoch 474/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1827 - accuracy: 0.9475\n",
            "Epoch 475/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1921 - accuracy: 0.9461\n",
            "Epoch 476/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1515 - accuracy: 0.9555\n",
            "Epoch 477/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1315 - accuracy: 0.9600\n",
            "Epoch 478/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.2117 - accuracy: 0.9402\n",
            "Epoch 479/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1843 - accuracy: 0.9477\n",
            "Epoch 480/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1517 - accuracy: 0.9554\n",
            "Epoch 481/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1641 - accuracy: 0.9516\n",
            "Epoch 482/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1988 - accuracy: 0.9432\n",
            "Epoch 483/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1797 - accuracy: 0.9487\n",
            "Epoch 484/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1539 - accuracy: 0.9539\n",
            "Epoch 485/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1759 - accuracy: 0.9490\n",
            "Epoch 486/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1608 - accuracy: 0.9547\n",
            "Epoch 487/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1564 - accuracy: 0.9556\n",
            "Epoch 488/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1796 - accuracy: 0.9490\n",
            "Epoch 489/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1685 - accuracy: 0.9516\n",
            "Epoch 490/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1989 - accuracy: 0.9441\n",
            "Epoch 491/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1652 - accuracy: 0.9535\n",
            "Epoch 492/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1761 - accuracy: 0.9495\n",
            "Epoch 493/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1689 - accuracy: 0.9508\n",
            "Epoch 494/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1640 - accuracy: 0.9530\n",
            "Epoch 495/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1458 - accuracy: 0.9575\n",
            "Epoch 496/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.2005 - accuracy: 0.9444\n",
            "Epoch 497/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1843 - accuracy: 0.9462\n",
            "Epoch 498/500\n",
            "825/825 [==============================] - 10s 13ms/step - loss: 0.1419 - accuracy: 0.9581\n",
            "Epoch 499/500\n",
            "825/825 [==============================] - 10s 12ms/step - loss: 0.1774 - accuracy: 0.9478\n",
            "Epoch 500/500\n",
            "825/825 [==============================] - 11s 13ms/step - loss: 0.1912 - accuracy: 0.9446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb466a52ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoF2aFAn0vJy"
      },
      "source": [
        "# saving model weights to allow loading for predictions at later date\r\n",
        "model.save_weights('/content/drive/MyDrive/Poirot.h5')"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuxipYT29JkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86fe094b-ebb6-4786-ff5f-c6dc15bc549a"
      },
      "source": [
        "#loading model weights from previous training run\r\n",
        "model = model.load_weights('/content/drive/MyDrive/Poirot.h5')\r\n",
        "print('--------------------')\r\n",
        "print('model weights loaded')\r\n",
        "print('--------------------')"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "model weights loaded\n",
            "--------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wnvauEwRD54",
        "outputId": "e7391019-2f05-4636-a48c-992c679298a6"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 35)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      multiple                  638100    \n",
            "_________________________________________________________________\n",
            "lstm_20 (LSTM)               (None, 35, 128)           117248    \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, 35, 128)           131584    \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 6380)              823020    \n",
            "=================================================================\n",
            "Total params: 1,858,048\n",
            "Trainable params: 1,219,948\n",
            "Non-trainable params: 638,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA8CezbsLl_e"
      },
      "source": [
        "#loading the encoder used previously\r\n",
        "encoder = load(open('encoder.pkl', 'rb'))\r\n",
        "print('--------------------')\r\n",
        "print('encoder loaded')\r\n",
        "print('--------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouDbiUr-t6dI"
      },
      "source": [
        "import builtins\r\n",
        "def predict_next():\r\n",
        "  #taking input as seed\r\n",
        "  seed=str(builtins.input('please enter some words (preferably 6 or more) as input to the model: '))\r\n",
        "  #cleaning input seed in same way as training text cleaned\r\n",
        "  seed=text_to_word_sequence(seed, filters='“”•!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\r\n",
        "\r\n",
        "  #displaying seed\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "  print(f'input_seed = {seed}')\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "\r\n",
        "  #encoding seed with encoder used for training\r\n",
        "  seed_encoded = encoder.texts_to_sequences([seed])[0]\r\n",
        "  #padding the seed so that inputs of varying sizes become 3 (longer takes last 3 words)\r\n",
        "  seed_encoded = pad_sequences([seed_encoded], maxlen=sequence_len, truncating='pre')\r\n",
        "  #prdicting next word using model\r\n",
        "  next = np.argmax(model.predict(seed_encoded), axis=-1)\r\n",
        "  next_word = ''\r\n",
        "  for word, index in encoder.word_index.items():\r\n",
        "    if index == next:\r\n",
        "      next_word = word\r\n",
        "      break\r\n",
        "  print(f'Generated next word: *{next_word}*')\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "  print(f'New text: {\" \".join(seed)} *{next_word}*')\r\n",
        "\r\n",
        "def generate_text():\r\n",
        "  #defining empty list for text to be stored in iteratively\r\n",
        "  text = list()\r\n",
        "  #taking input as seed\r\n",
        "  input_seed = str(builtins.input('please enter some words (preferably 6 or more) as input to the model: '))\r\n",
        "  #use input of number of words for generated sentence (validated)\r\n",
        "  while True: \r\n",
        "      try:\r\n",
        "          text_len = int(builtins.input(\"Please enter a number for the length of text to be generated: \"))\r\n",
        "          break\r\n",
        "      except ValueError:\r\n",
        "          print(\"Please enter a number...\")\r\n",
        "  #cleaning input seed in same way as training text cleaned\r\n",
        "  seed=text_to_word_sequence(input_seed, filters='“”•!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\r\n",
        "\r\n",
        "  #displaying seed input\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "  print(f'input seed = {input_seed}')\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "\r\n",
        "  # iteratively predicting and appending new words to seed to generate text\r\n",
        "  for i in range(text_len):\r\n",
        "    #encoding seed using generator used previously\r\n",
        "    seed_encoded = encoder.texts_to_sequences([seed])[0]\r\n",
        "    seed_encoded = pad_sequences([seed_encoded], maxlen=sequence_len, truncating='pre')\r\n",
        "    next = np.argmax(model.predict(seed_encoded), axis=-1)\r\n",
        "    next_word = ''\r\n",
        "    for word, index in encoder.word_index.items():\r\n",
        "      if index == next:\r\n",
        "        next_word = word\r\n",
        "        break\r\n",
        "    seed += ' ' + ''.join(next_word)\r\n",
        "    text.append(next_word)\r\n",
        "  print(f'Generated next word: *{text}*')\r\n",
        "  print('--------------------------------------------------------------')\r\n",
        "  print(f\"\"\"New text: {input_seed + \" \" + \" \".join(text)}\"\"\")"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KYBShjIrbYy",
        "outputId": "623acb95-d2db-48fb-a9d4-6983349ae948"
      },
      "source": [
        "predict_next()"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "please enter some words (preferably 6 or more) as input to the model: As usual, Poirot was right. After a short interval, the American film star was ushered in, and we rose\n",
            "--------------------------------------------------------------\n",
            "input_seed = ['as', 'usual', 'poirot', 'was', 'right', 'after', 'a', 'short', 'interval', 'the', 'american', 'film', 'star', 'was', 'ushered', 'in', 'and', 'we', 'rose']\n",
            "--------------------------------------------------------------\n",
            "Generated next word: *him*\n",
            "--------------------------------------------------------------\n",
            "New text: as usual poirot was right after a short interval the american film star was ushered in and we rose *him*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmGYIzbbrcty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3460064e-dd10-4eb7-f850-c2cc8d76bdc3"
      },
      "source": [
        "generate_text()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "please enter some words (preferably 6 or more) as input to the model: I looked with some curiosity at “Alfred darling”. He certainly struck a rather alien note. I did not wonder at John objecting to his beard. It was one of the\n",
            "Please enter a number for the length of text to be generated: 20\n",
            "--------------------------------------------------------------\n",
            "input seed = I looked with some curiosity at “Alfred darling”. He certainly struck a rather alien note. I did not wonder at John objecting to his beard. It was one of the\n",
            "--------------------------------------------------------------\n",
            "Generated next word: *['greatest', 'left', 'with', 'see', 'these', 'i', 'pray', 'little', 'with', 'suggested', 'précisément', 'tosswill', 'pretty', 'and', 'that', 'shame', 'but', 'that', 'pair', 'gently']*\n",
            "--------------------------------------------------------------\n",
            "New text: I looked with some curiosity at “Alfred darling”. He certainly struck a rather alien note. I did not wonder at John objecting to his beard. It was one of the greatest left with see these i pray little with suggested précisément tosswill pretty and that shame but that pair gently\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XSNd_6MJkMuS",
        "outputId": "9cecbc08-75a1-446a-b49d-5e975af4c40b"
      },
      "source": [
        "sequences[361]"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'know it well you really have the best opinion of yourself of anyone i ever knew i cried divided between amusement and annoyance what will you when one is unique one knows it and others share'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXZjbldidZiA"
      },
      "source": [
        "https://keras.io/examples/nlp/pretrained_word_embeddings/"
      ]
    }
  ]
}