# Word-Level-Language-Model-Poirot
Word-level language model (Recurrent Neural Network: LSTM) trained on Agatha Christie's 'Poirot Investigates'

The aim of this model was to generate text when prompted by the user. The model was trained to take an input of 35 words and output any number of words specified by the user. A function was carefully designed to take interactive input from the user, validate the input and return the generated text of the length specified. The function zero pads an input of less than 35 words and predicts on the last 35 words which allows any number of words to be input, however inputting less than 35 words will likely result in less than optimal results. Smaller training sequences were experimented with but were giving very poor results that seemed no better than random and so the trade-off of larger inputs was accepted.
A recurrent network using stacked LSTM layers was decided upon. LSTM was chosen over a vanilla RNN since the input is a fairly long sequence of words and it’s important to take into account words at the beginning of the sequence as well as at the end (vanilla RNN has a vanishing gradient across a sequence and ‘forgets’ the beginning of a sequence’). Hyperparameter tuning was attempted using Keras tuner and leaving out a validation set, although of course this is not optimal as accuracy is a rather arbitrary metric in this type of model. The tuner object was lost as it wasn’t saved, but the hyperparameters were defined in the main model and so this didn’t matter greatly. The tuner chose a model with three stacked LSTM layers with 128 units each, and then a dense layer with 128 units. Categorical cross entropy was chosen as the loss function for the model since this is essentially a multi-class classification problem. Training this model across varying numbers of epochs (30, 50, 100, 300) produced poor results, possibly because the data is quite small for this kind of model and so the model may not learn useful word embeddings. To try to mitigate this, GloVe embeddings pre-trained on the Newsgroup20 dataset were loaded and mapped to the original vectors to create a new embedding layer. This new model was then trained across 200 epochs.
The results were slightly disappointing even with the addition of the pre-trained embedding layer, with most generated text not making much sense (although in consolation the Poirot text without punctuation doesn’t make much sense to me either). The next word prediction is not too bad however, usually providing a plausible next word for the first output.
Much more data would likely have helped to train a better model, and perhaps word embeddings pre-trained on a more relevant dataset would have been better. In further development I would also look at better cleaning of the text, perhaps filtering out some of the French, as often the model outputs French words mixed in with English.
